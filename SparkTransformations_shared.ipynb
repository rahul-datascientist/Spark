{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to be loaded everytime when Jupyter Notebook starts\n",
    "from pyspark import SparkContext\n",
    "sc =SparkContext()\n",
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', 'a', 'c']\n",
      "[('b', 1), ('a', 1), ('c', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Example of map\n",
    "x = sc.parallelize([\"b\", \"a\", \"c\"]) \n",
    "y = x.map(lambda z: (z, 1))\n",
    "print(x.collect())\n",
    "print(y.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[1, 3]\n"
     ]
    }
   ],
   "source": [
    "# Use of filter\n",
    "x = sc.parallelize([1,2,3])\n",
    "y = x.filter(lambda x: x%2 == 1) #keep odd values\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[1, 100, 42, 2, 200, 42, 3, 300, 42]\n"
     ]
    }
   ],
   "source": [
    "#example of flatMap\n",
    "x = sc.parallelize([1,2,3]) \n",
    "y = x.flatMap(lambda x: (x, x*100, 42))\n",
    "print(x.collect())\n",
    "print(y.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('J', <pyspark.resultiterable.ResultIterable object at 0x7febd53a0278>), ('F', <pyspark.resultiterable.ResultIterable object at 0x7febd53a0780>), ('A', <pyspark.resultiterable.ResultIterable object at 0x7febd53a0128>)]\n",
      "-------------------------\n",
      "[('J', ['John', 'James']), ('F', ['Fred']), ('A', ['Anna'])]\n"
     ]
    }
   ],
   "source": [
    "#example of groupBy\n",
    "x = sc.parallelize(['John', 'Fred', 'Anna', 'James'])\n",
    "y = x.groupBy(lambda w: w[0])\n",
    "print (y.collect()) \n",
    "print(\"-------------------------\")\n",
    "# will print a object and hence a detailed print statement below\n",
    "print ([(k, list(v)) for (k, v) in y.collect()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', 5), ('B', 4), ('A', 3), ('A', 2), ('A', 1)]\n",
      "[('B', [5, 4]), ('A', [3, 2, 1])]\n"
     ]
    }
   ],
   "source": [
    "# Example of groupByKey\n",
    "x = sc.parallelize([('B',5),('B',4),('A',3),('A',2),('A',1)])\n",
    "y = x.groupByKey()\n",
    "print(x.collect())\n",
    "print(list((j[0], list(j[1])) for j in y.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]\n",
      "[[15], [40]]\n"
     ]
    }
   ],
   "source": [
    "# example of MapPartitions\n",
    "x = sc.parallelize([1,2,3,4,5,6,7,8,9,10], 2)\n",
    "\n",
    "def f(iterator): yield sum(iterator);\n",
    "\n",
    "y = x.mapPartitions(f)\n",
    "\n",
    "# glom() flattens elements on the same partition\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n",
      "[1, 2]\n"
     ]
    }
   ],
   "source": [
    "#Example of sample\n",
    "'''\n",
    "withReplacement - can elements be sampled multiple times \n",
    "(replaced when sampled out)\n",
    "\n",
    "fraction - expected size of the sample as a fraction of this\n",
    "RDD's size without replacement: probability that each element is chosen; fraction must be [0, 1] with replacement: expected number of times each element is chosen; fraction must be >= 0\n",
    "\n",
    "seed - seed for the random number generator\n",
    "'''\n",
    "\n",
    "x = sc.parallelize([1, 2, 3, 4, 5])\n",
    "y = x.sample(False, 0.4, 42)\n",
    "print(x.collect())\n",
    "print(y.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2, 3]]\n",
      "[[3, 4]]\n",
      "[[1], [2, 3], [3, 4]]\n"
     ]
    }
   ],
   "source": [
    "# union here is like union all in RDBMS\n",
    "x = sc.parallelize([1,2,3], 2)\n",
    "print (x.glom().collect())\n",
    "y = sc.parallelize([3,4], 1)\n",
    "print (y.glom().collect())\n",
    "z = x.union(y)\n",
    "print(z.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('a', 1)], [('b', 2)]]\n",
      "[[('a', 3)], [('a', 4), ('b', 5)]]\n",
      "[[('b', (2, 5))], [('a', (1, 3)), ('a', (1, 4))]]\n"
     ]
    }
   ],
   "source": [
    "# join will work only with pairedRDDs\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 2)])\n",
    "y = sc.parallelize([(\"a\", 3), (\"a\", 4), (\"b\", 5)])\n",
    "z = x.join(y,2)\n",
    "#print(x.getNumPartitions())\n",
    "#print(y.getNumPartitions())\n",
    "#print(z.getNumPartitions())\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())\n",
    "print(z.glom().collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2], [3, 3, 4]]\n",
      "[[2, 4], [1, 3]]\n"
     ]
    }
   ],
   "source": [
    "#distinct\n",
    "x = sc.parallelize([1,2,3,3,4])\n",
    "y = x.distinct()\n",
    "\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2, 3], [4, 5]]\n",
      "[[1, 2, 4], [3, 5]]\n"
     ]
    }
   ],
   "source": [
    "#coalese --> reduce the number of partitions\n",
    "# if shuffle = False, which is the default\n",
    "# if shuffle = True, then there would be a movement \n",
    "# of elements in one partition to another partition - new Stage \n",
    "x = sc.parallelize([1, 2, 3, 4, 5], 3)\n",
    "y = x.coalesce(2,shuffle=True)\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('J', 'John'), ('F', 'Fred'), ('A', 'Anna'), ('J', 'James')]\n"
     ]
    }
   ],
   "source": [
    "# keyBy Example\n",
    "x = sc.parallelize(['John', 'Fred', 'Anna', 'James'])\n",
    "y = x.keyBy(lambda w: w[0])\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('J', 'James')], [('F', 'Fred')], [('A', 'Anna'), ('J', 'John')]]\n",
      "[[('F', 'Fred'), ('A', 'Anna')], [('J', 'James'), ('J', 'John')]]\n"
     ]
    }
   ],
   "source": [
    "#partitionBy example\n",
    "x = sc.parallelize([('J','James'),('F','Fred'),\n",
    "\t\t\t\t ('A','Anna'),('J','John')], 3)\n",
    "\n",
    "y = x.partitionBy(2, lambda w: 0 if w[0] < 'H' else 1)\n",
    "print (x.glom().collect())\n",
    "print (y.glom().collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (2, 4), (3, 9)]\n"
     ]
    }
   ],
   "source": [
    "#keyBy Example --> typical usecase of fuse in ETL\n",
    "x = sc.parallelize([1, 2, 3])\n",
    "y = x.map(lambda n:n*n)\n",
    "z = x.zip(y)\n",
    "\n",
    "print(z.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# reduce is a action and not a transformation as it returns the result to \n",
    "# the driver\n",
    "x = sc.parallelize([1,2,3,4])\n",
    "y = x.reduce(lambda a,b: a+b)\n",
    "\n",
    "print(x.collect())\n",
    "print(y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
