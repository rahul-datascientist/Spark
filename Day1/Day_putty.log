login as: notroot
notroot@192.168.150.143's password:
Access denied
notroot@192.168.150.143's password:
Welcome to Ubuntu 14.04.1 LTS (GNU/Linux 3.13.0-32-generic x86_64)

 * Documentation:  https://help.ubuntu.com/
New release '16.04.5 LTS' available.
Run 'do-release-upgrade' to upgrade to it.

Last login: Sun Oct 28 22:31:33 2018
notroot@ubuntu:~$ start-dfs.sh
Starting namenodes on [localhost]
localhost: starting namenode, logging to /home/notroot/lab/software/hadoop-2.7.2/logs/hadoop-notroot-na
localhost: starting datanode, logging to /home/notroot/lab/software/hadoop-2.7.2/logs/hadoop-notroot-da
Starting secondary namenodes [0.0.0.0]
0.0.0.0: starting secondarynamenode, logging to /home/notroot/lab/software/hadoop-2.7.2/logs/hadoop-not
notroot@ubuntu:~$ jps
1567 NameNode
1884 SecondaryNameNode
1999 Jps
1706 DataNode
notroot@ubuntu:~$ start-yarn.sh
starting yarn daemons
starting resourcemanager, logging to /home/notroot/lab/software/hadoop-2.7.2/logs/yarn-notroot-resource
localhost: starting nodemanager, logging to /home/notroot/lab/software/hadoop-2.7.2/logs/yarn-notroot-n
notroot@ubuntu:~$ jps
1567 NameNode
2066 ResourceManager
1884 SecondaryNameNode
2179 NodeManager
1706 DataNode
2471 Jps
notroot@ubuntu:~$ mr-jobhistory-daemon.sh start historyserver
starting historyserver, logging to /home/notroot/lab/software/hadoop-2.7.2/logs/mapred-notroot-historys
notroot@ubuntu:~$ jos
No command 'jos' found, did you mean:
 Command 'bos' from package 'openafs-client' (universe)
 Command 'jls' from package 'sleuthkit' (universe)
 Command 'josm' from package 'josm' (universe)
 Command 'jot' from package 'athena-jot' (universe)
 Command 'eos' from package 'elk-lapw' (universe)
 Command 'jps' from package 'openjdk-7-jdk' (main)
 Command 'jps' from package 'openjdk-6-jdk' (universe)
 Command 'js' from package 'nodejs' (universe)
 Command 'js' from package 'rhino' (main)
 Command 'vos' from package 'openafs-client' (universe)
 Command 'joe' from package 'joe' (universe)
 Command 'joe' from package 'joe-jupp' (universe)
jos: command not found
notroot@ubuntu:~$ jps
2511 JobHistoryServer
1567 NameNode
2066 ResourceManager
1884 SecondaryNameNode
2179 NodeManager
2551 Jps
1706 DataNode
notroot@ubuntu:~$ spark-shell\
> \
>

^Cnotroot@ubuntu:~$ spark-shell
18/10/28 23:47:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform...
18/10/28 23:47:54 INFO spark.SecurityManager: Changing view acls to: notroot
18/10/28 23:47:54 INFO spark.SecurityManager: Changing modify acls to: notroot
18/10/28 23:47:54 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disableermissions: Set(notroot)
18/10/28 23:47:54 INFO spark.HttpServer: Starting HTTP Server
18/10/28 23:47:54 INFO server.Server: jetty-8.y.z-SNAPSHOT
18/10/28 23:47:54 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:37312
18/10/28 23:47:54 INFO util.Utils: Successfully started service 'HTTP class server' on port 37312.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.6.2
      /_/

Using Scala version 2.10.5 (OpenJDK 64-Bit Server VM, Java 1.7.0_79)
Type in expressions to have them evaluated.
Type :help for more information.
18/10/28 23:48:00 WARN util.Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; usi
18/10/28 23:48:00 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/10/28 23:48:00 INFO spark.SparkContext: Running Spark version 1.6.2
18/10/28 23:48:00 INFO spark.SecurityManager: Changing view acls to: notroot
18/10/28 23:48:00 INFO spark.SecurityManager: Changing modify acls to: notroot
18/10/28 23:48:00 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disableermissions: Set(notroot)
18/10/28 23:48:00 INFO util.Utils: Successfully started service 'sparkDriver' on port 53290.
18/10/28 23:48:01 INFO slf4j.Slf4jLogger: Slf4jLogger started
18/10/28 23:48:01 INFO Remoting: Starting remoting
18/10/28 23:48:01 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActor
18/10/28 23:48:01 INFO util.Utils: Successfully started service 'sparkDriverActorSystem' on port 60941.
18/10/28 23:48:01 INFO spark.SparkEnv: Registering MapOutputTracker
18/10/28 23:48:01 INFO spark.SparkEnv: Registering BlockManagerMaster
18/10/28 23:48:01 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-b163a181-6f41
18/10/28 23:48:01 INFO storage.MemoryStore: MemoryStore started with capacity 511.5 MB
18/10/28 23:48:01 INFO spark.SparkEnv: Registering OutputCommitCoordinator
18/10/28 23:48:02 INFO server.Server: jetty-8.y.z-SNAPSHOT
18/10/28 23:48:02 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040
18/10/28 23:48:02 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
18/10/28 23:48:02 INFO ui.SparkUI: Started SparkUI at http://192.168.150.143:4040
18/10/28 23:48:02 INFO executor.Executor: Starting executor ID driver on host localhost
18/10/28 23:48:02 INFO executor.Executor: Using REPL class URI: http://192.168.150.143:37312
18/10/28 23:48:02 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBl
18/10/28 23:48:02 INFO netty.NettyBlockTransferService: Server created on 39802
18/10/28 23:48:02 INFO storage.BlockManagerMaster: Trying to register BlockManager
18/10/28 23:48:02 INFO storage.BlockManagerMasterEndpoint: Registering block manager localhost:39802 wi
18/10/28 23:48:02 INFO storage.BlockManagerMaster: Registered BlockManager
18/10/28 23:48:02 INFO repl.SparkILoop: Created spark context..
Spark context available as sc.
18/10/28 23:48:03 INFO hive.HiveContext: Initializing execution hive, version 1.2.1
18/10/28 23:48:03 INFO client.ClientWrapper: Inspected Hadoop version: 2.6.0
18/10/28 23:48:03 INFO client.ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hado
18/10/28 23:48:04 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apach
18/10/28 23:48:04 INFO metastore.ObjectStore: ObjectStore, initialize called
18/10/28 23:48:04 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ign
18/10/28 23:48:04 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown -
18/10/28 23:48:05 WARN DataNucleus.Connection: BoneCP specified but not present in CLASSPATH (or one of
18/10/28 23:48:05 WARN DataNucleus.Connection: BoneCP specified but not present in CLASSPATH (or one of
18/10/28 23:48:07 INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.tabase,Type,FieldSchema,Order"
18/10/28 23:48:08 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSstore table.
18/10/28 23:48:08 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder"table.
18/10/28 23:48:10 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSstore table.
18/10/28 23:48:10 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder"table.
18/10/28 23:48:10 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
18/10/28 23:48:10 INFO metastore.ObjectStore: Initialized ObjectStore
18/10/28 23:48:10 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastorion 1.2.0
18/10/28 23:48:10 WARN metastore.ObjectStore: Failed to get database default, returning NoSuchObjectExc
18/10/28 23:48:11 INFO metastore.HiveMetaStore: Added admin role in metastore
18/10/28 23:48:11 INFO metastore.HiveMetaStore: Added public role in metastore
18/10/28 23:48:11 INFO metastore.HiveMetaStore: No user is added in admin role, since config is empty
18/10/28 23:48:11 INFO metastore.HiveMetaStore: 0: get_all_databases
18/10/28 23:48:11 INFO HiveMetaStore.audit: ugi=notroot ip=unknown-ip-addr      cmd=get_all_databases
18/10/28 23:48:11 INFO metastore.HiveMetaStore: 0: get_functions: db=default pat=*
18/10/28 23:48:11 INFO HiveMetaStore.audit: ugi=notroot ip=unknown-ip-addr      cmd=get_functions: db=d
18/10/28 23:48:11 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourstore table.
18/10/28 23:48:12 INFO session.SessionState: Created HDFS directory: /tmp/hive/notroot
18/10/28 23:48:12 INFO session.SessionState: Created local directory: /tmp/notroot
18/10/28 23:48:12 INFO session.SessionState: Created local directory: /tmp/876d4539-73b9-4160-9138-3136
18/10/28 23:48:12 INFO session.SessionState: Created HDFS directory: /tmp/hive/notroot/876d4539-73b9-41
18/10/28 23:48:12 INFO session.SessionState: Created local directory: /tmp/notroot/876d4539-73b9-4160-9
18/10/28 23:48:12 INFO session.SessionState: Created HDFS directory: /tmp/hive/notroot/876d4539-73b9-41
18/10/28 23:48:12 INFO hive.HiveContext: default warehouse location is /user/hive/warehouse
18/10/28 23:48:12 INFO hive.HiveContext: Initializing HiveMetastoreConnection version 1.2.1 using Spark
18/10/28 23:48:12 INFO client.ClientWrapper: Inspected Hadoop version: 2.6.0
18/10/28 23:48:12 INFO client.ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hado
18/10/28 23:48:13 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apach
18/10/28 23:48:13 INFO metastore.ObjectStore: ObjectStore, initialize called
18/10/28 23:48:13 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ign
18/10/28 23:48:13 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown -
18/10/28 23:48:14 WARN DataNucleus.Connection: BoneCP specified but not present in CLASSPATH (or one of
18/10/28 23:48:14 WARN DataNucleus.Connection: BoneCP specified but not present in CLASSPATH (or one of
18/10/28 23:48:15 INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.tabase,Type,FieldSchema,Order"
18/10/28 23:48:16 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSstore table.
18/10/28 23:48:16 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder"table.
18/10/28 23:48:18 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSstore table.
18/10/28 23:48:18 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder"table.
18/10/28 23:48:18 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
18/10/28 23:48:18 INFO metastore.ObjectStore: Initialized ObjectStore
18/10/28 23:48:18 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastorion 1.2.0
18/10/28 23:48:18 WARN metastore.ObjectStore: Failed to get database default, returning NoSuchObjectExc
18/10/28 23:48:18 INFO metastore.HiveMetaStore: Added admin role in metastore
18/10/28 23:48:18 INFO metastore.HiveMetaStore: Added public role in metastore
18/10/28 23:48:18 INFO metastore.HiveMetaStore: No user is added in admin role, since config is empty
18/10/28 23:48:18 INFO metastore.HiveMetaStore: 0: get_all_databases
18/10/28 23:48:18 INFO HiveMetaStore.audit: ugi=notroot ip=unknown-ip-addr      cmd=get_all_databases
18/10/28 23:48:18 INFO metastore.HiveMetaStore: 0: get_functions: db=default pat=*
18/10/28 23:48:18 INFO HiveMetaStore.audit: ugi=notroot ip=unknown-ip-addr      cmd=get_functions: db=d
18/10/28 23:48:18 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourstore table.
18/10/28 23:48:19 INFO session.SessionState: Created local directory: /tmp/17f4a4e3-50cc-4350-b2db-103c
18/10/28 23:48:19 INFO session.SessionState: Created HDFS directory: /tmp/hive/notroot/17f4a4e3-50cc-43
18/10/28 23:48:19 INFO session.SessionState: Created local directory: /tmp/notroot/17f4a4e3-50cc-4350-b
18/10/28 23:48:19 INFO session.SessionState: Created HDFS directory: /tmp/hive/notroot/17f4a4e3-50cc-43
18/10/28 23:48:19 INFO repl.SparkILoop: Created sql context (with Hive support)..
SQL context available as sqlContext.

scala> sc.
accumulable                    accumulableCollection          accumulator                    addFile
addSparkListener               appName                        applicationAttemptId           applicatio
binaryFiles                    binaryRecords                  broadcast                      cancelAllJ
clearCallSite                  clearFiles                     clearJars                      clearJobGr
defaultMinSplits               defaultParallelism             emptyRDD                       externalBl
getAllPools                    getCheckpointDir               getConf                        getExecuto
getLocalProperty               getPersistentRDDs              getPoolForName                 getRDDStor
hadoopConfiguration            hadoopFile                     hadoopRDD                      initLocalP
isLocal                        isStopped                      jars                           killExecut
makeRDD                        master                         metricsSystem                  newAPIHado
objectFile                     parallelize                    range                          requestExe
runJob                         sequenceFile                   setCallSite                    setCheckpo
setJobGroup                    setLocalProperty               setLogLevel                    sparkUser
statusTracker                  stop                           submitJob                      tachyonFol
toString                       union                          version                        wholeTextF

scala> val fname = sc.
accumulable                    accumulableCollection          accumulator                    addFile
addSparkListener               appName                        applicationAttemptId           applicatio
binaryFiles                    binaryRecords                  broadcast                      cancelAllJ
clearCallSite                  clearFiles                     clearJars                      clearJobGr
defaultMinSplits               defaultParallelism             emptyRDD                       externalBl
getAllPools                    getCheckpointDir               getConf                        getExecuto
getLocalProperty               getPersistentRDDs              getPoolForName                 getRDDStor
hadoopConfiguration            hadoopFile                     hadoopRDD                      initLocalP
isLocal                        isStopped                      jars                           killExecut
makeRDD                        master                         metricsSystem                  newAPIHado
objectFile                     parallelize                    range                          requestExe
runJob                         sequenceFile                   setCallSite                    setCheckpo
setJobGroup                    setLocalProperty               setLogLevel                    sparkUser
statusTracker                  stop                           submitJob                      tachyonFol
toString                       union                          version                        wholeTextF

scala> val fname = sc.textFile("file:///home/notroot/sample")
18/10/29 00:41:39 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated siz
18/10/29 00:41:39 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimat
18/10/29 00:41:39 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:39802
18/10/29 00:41:39 INFO spark.SparkContext: Created broadcast 0 from textFile at <console>:27
fname: org.apache.spark.rdd.RDD[String] = file:///home/notroot/sample MapPartitionsRDD[1] at textFile a

scala> val fname = sc.textFile("file:///home/notroot/lab/data/sample")
18/10/29 00:42:17 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated siz
18/10/29 00:42:17 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimat
18/10/29 00:42:17 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:39802
18/10/29 00:42:17 INFO spark.SparkContext: Created broadcast 1 from textFile at <console>:27
fname: org.apache.spark.rdd.RDD[String] = file:///home/notroot/lab/data/sample MapPartitionsRDD[3] at t

scala> fname.collect()
18/10/29 00:47:34 INFO mapred.FileInputFormat: Total input paths to process : 1
18/10/29 00:47:34 INFO spark.SparkContext: Starting job: collect at <console>:30
18/10/29 00:47:34 INFO scheduler.DAGScheduler: Got job 0 (collect at <console>:30) with 2 output partit
18/10/29 00:47:34 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (collect at <console>:30)
18/10/29 00:47:34 INFO scheduler.DAGScheduler: Parents of final stage: List()
18/10/29 00:47:34 INFO scheduler.DAGScheduler: Missing parents: List()
18/10/29 00:47:34 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (file:///home/notroot/lab/data/s no missing parents
18/10/29 00:47:34 INFO storage.MemoryStore: Block broadcast_2 stored as values in memory (estimated siz
18/10/29 00:47:34 INFO storage.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimat
18/10/29 00:47:34 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:39802
18/10/29 00:47:34 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:100
18/10/29 00:47:34 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (file:///honsole>:27)
18/10/29 00:47:34 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
18/10/29 00:47:34 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, part
18/10/29 00:47:34 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, part
18/10/29 00:47:34 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
18/10/29 00:47:34 INFO executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
18/10/29 00:47:34 INFO rdd.HadoopRDD: Input split: file:/home/notroot/lab/data/sample:0+18
18/10/29 00:47:34 INFO rdd.HadoopRDD: Input split: file:/home/notroot/lab/data/sample:18+19
18/10/29 00:47:34 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.t
18/10/29 00:47:34 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapred
18/10/29 00:47:34 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use map
18/10/29 00:47:34 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.j
18/10/29 00:47:34 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.
18/10/29 00:47:34 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2070 bytes result sen
18/10/29 00:47:34 INFO executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 2060 bytes result sen
18/10/29 00:47:34 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 121 ms on lo
18/10/29 00:47:34 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 117 ms on lo
18/10/29 00:47:34 INFO scheduler.DAGScheduler: ResultStage 0 (collect at <console>:30) finished in 0.14
18/10/29 00:47:34 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed
18/10/29 00:47:34 INFO scheduler.DAGScheduler: Job 0 finished: collect at <console>:30, took 0.399483 s
res5: Array[String] = Array(How are you, I am fine, How about you)

scala> fname.g18/10/29 00:47:47 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on localhost:
18/10/29 00:47:47 INFO spark.ContextCleaner: Cleaned accumulator 1
             g
getCheckpointFile   getNumPartitions    getStorageLevel     glom                groupBy

scala> fname.glom.collect()
18/10/29 00:47:57 INFO spark.SparkContext: Starting job: collect at <console>:30
18/10/29 00:47:57 INFO scheduler.DAGScheduler: Got job 1 (collect at <console>:30) with 2 output partit
18/10/29 00:47:57 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (collect at <console>:30)
18/10/29 00:47:57 INFO scheduler.DAGScheduler: Parents of final stage: List()
18/10/29 00:47:57 INFO scheduler.DAGScheduler: Missing parents: List()
18/10/29 00:47:57 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at glom at
18/10/29 00:47:57 INFO storage.MemoryStore: Block broadcast_3 stored as values in memory (estimated siz
18/10/29 00:47:57 INFO storage.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimat
18/10/29 00:47:57 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:39802
18/10/29 00:47:57 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:100
18/10/29 00:47:57 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartit
18/10/29 00:47:57 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
18/10/29 00:47:57 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, localhost, part
18/10/29 00:47:57 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, localhost, part
18/10/29 00:47:57 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 2)
18/10/29 00:47:57 INFO executor.Executor: Running task 1.0 in stage 1.0 (TID 3)
18/10/29 00:47:57 INFO rdd.HadoopRDD: Input split: file:/home/notroot/lab/data/sample:18+19
18/10/29 00:47:57 INFO rdd.HadoopRDD: Input split: file:/home/notroot/lab/data/sample:0+18
18/10/29 00:47:57 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 2). 2111 bytes result sen
18/10/29 00:47:57 INFO executor.Executor: Finished task 1.0 in stage 1.0 (TID 3). 2101 bytes result sen
18/10/29 00:47:57 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 26 ms on loc
18/10/29 00:47:57 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 27 ms on loc
18/10/29 00:47:57 INFO scheduler.DAGScheduler: ResultStage 1 (collect at <console>:30) finished in 0.01
18/10/29 00:47:57 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed
18/10/29 00:47:57 INFO scheduler.DAGScheduler: Job 1 finished: collect at <console>:30, took 0.040959 s
res6: Array[Array[String]] = Array(Array(How are you, I am fine), Array(How about you))

scala> 18/10/29 00:48:03 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on localhost:39802 i
18/10/29 00:48:03 INFO spark.ContextCleaner: Cleaned accumulator 2


scala> val fname1 = fname.flatMap(sou => sou.split(“ “))
<console>:1: error: illegal character '\u201c'
       val fname1 = fname.flatMap(sou => sou.split(“ “))
                                                   ^
<console>:1: error: illegal character '\u201c'
       val fname1 = fname.flatMap(sou => sou.split(“ “))
                                                     ^

scala> fname1.collect()
<console>:26: error: not found: value fname1
              fname1.collect()
              ^

scala> val fname1 = fname.flatMap(sou => sou.split(" "))
fname1: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[5] at flatMap at <console>:29

scala> fname1.collect()
18/10/29 01:13:40 INFO spark.SparkContext: Starting job: collect at <console>:32
18/10/29 01:13:40 INFO scheduler.DAGScheduler: Got job 2 (collect at <console>:32) with 2 output partit
18/10/29 01:13:40 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (collect at <console>:32)
18/10/29 01:13:40 INFO scheduler.DAGScheduler: Parents of final stage: List()
18/10/29 01:13:40 INFO scheduler.DAGScheduler: Missing parents: List()
18/10/29 01:13:40 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[5] at flatMap
18/10/29 01:13:40 INFO storage.MemoryStore: Block broadcast_4 stored as values in memory (estimated siz
18/10/29 01:13:40 INFO storage.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimat
18/10/29 01:13:40 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:39802
18/10/29 01:13:40 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:100
18/10/29 01:13:40 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (MapPartit
18/10/29 01:13:40 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 2 tasks
18/10/29 01:13:40 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, localhost, part
18/10/29 01:13:40 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, localhost, part
18/10/29 01:13:40 INFO executor.Executor: Running task 1.0 in stage 2.0 (TID 5)
18/10/29 01:13:40 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 4)
18/10/29 01:13:40 INFO rdd.HadoopRDD: Input split: file:/home/notroot/lab/data/sample:0+18
18/10/29 01:13:40 INFO rdd.HadoopRDD: Input split: file:/home/notroot/lab/data/sample:18+19
18/10/29 01:13:40 INFO executor.Executor: Finished task 1.0 in stage 2.0 (TID 5). 2119 bytes result sen
18/10/29 01:13:40 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 4). 2133 bytes result sen
18/10/29 01:13:40 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 30 ms on loc
18/10/29 01:13:40 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 33 ms on loc
18/10/29 01:13:40 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed
18/10/29 01:13:40 INFO scheduler.DAGScheduler: ResultStage 2 (collect at <console>:32) finished in 0.03
18/10/29 01:13:40 INFO scheduler.DAGScheduler: Job 2 finished: collect at <console>:32, took 0.044008 s
res8: Array[String] = Array(How, are, you, I, am, fine, How, about, you)

scala> val fname2 = fname1.map(k => (k,1))
fname2: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[6] at map at <console>:31

scala> fname2.collect()
18/10/29 01:17:38 INFO spark.SparkContext: Starting job: collect at <console>:34
18/10/29 01:17:38 INFO scheduler.DAGScheduler: Got job 3 (collect at <console>:34) with 2 output partit
18/10/29 01:17:38 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (collect at <console>:34)
18/10/29 01:17:38 INFO scheduler.DAGScheduler: Parents of final stage: List()
18/10/29 01:17:38 INFO scheduler.DAGScheduler: Missing parents: List()
18/10/29 01:17:38 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[6] at map at
18/10/29 01:17:38 INFO storage.MemoryStore: Block broadcast_5 stored as values in memory (estimated siz
18/10/29 01:17:38 INFO storage.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimat
18/10/29 01:17:38 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:39802
18/10/29 01:17:38 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:100
18/10/29 01:17:38 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (MapPartit
18/10/29 01:17:38 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 2 tasks
18/10/29 01:17:38 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 6, localhost, part
18/10/29 01:17:38 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 7, localhost, part
18/10/29 01:17:38 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 6)
18/10/29 01:17:38 INFO executor.Executor: Running task 1.0 in stage 3.0 (TID 7)
18/10/29 01:17:38 INFO rdd.HadoopRDD: Input split: file:/home/notroot/lab/data/sample:0+18
18/10/29 01:17:38 INFO rdd.HadoopRDD: Input split: file:/home/notroot/lab/data/sample:18+19
18/10/29 01:17:38 INFO executor.Executor: Finished task 1.0 in stage 3.0 (TID 7). 2279 bytes result sen
18/10/29 01:17:38 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 6). 2329 bytes result sen
18/10/29 01:17:38 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 7) in 11 ms on loc
18/10/29 01:17:38 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 6) in 15 ms on loc
18/10/29 01:17:38 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed
18/10/29 01:17:38 INFO scheduler.DAGScheduler: ResultStage 3 (collect at <console>:34) finished in 0.01
18/10/29 01:17:38 INFO scheduler.DAGScheduler: Job 3 finished: collect at <console>:34, took 0.023811 s
res9: Array[(String, Int)] = Array((How,1), (are,1), (you,1), (I,1), (am,1), (fine,1), (How,1), (about,

scala> 18/10/29 01:18:03 INFO spark.ContextCleaner: Cleaned accumulator 3
18/10/29 01:18:03 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on localhost:39802 in memor
18/10/29 01:18:03 INFO spark.ContextCleaner: Cleaned accumulator 4
18/10/29 01:18:03 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on localhost:39802 in memor


scala> exit
warning: there were 1 deprecation warning(s); re-run with -deprecation for details
18/10/29 01:18:44 INFO spark.SparkContext: Invoking stop() from shutdown hook
18/10/29 01:18:44 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/static/sql,null}
18/10/29 01:18:44 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/SQL/execution/jso
18/10/29 01:18:44 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/SQL/execution,nul
18/10/29 01:18:44 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/SQL/json,null}
18/10/29 01:18:44 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/SQL,null}
18/10/29 01:18:44 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null
18/10/29 01:18:44 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill
18/10/29 01:18:44 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}
18/10/29 01:18:44 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
18/10/29 01:18:44 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
18/10/29 01:18:44 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadD
18/10/29 01:18:44 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadD
18/10/29 01:18:44 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,nu
18/10/29 01:18:44 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
18/10/29 01:18:44 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,
18/10/29 01:18:44 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
18/10/29 01:18:44 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,
18/10/29 01:18:44 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
18/10/29 01:18:44 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null
18/10/29 01:18:44 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}
18/10/29 01:18:44 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,
18/10/29 01:18:44 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
18/10/29 01:18:44 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json
18/10/29 01:18:44 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null
18/10/29 01:18:44 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
18/10/29 01:18:44 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}
18/10/29 01:18:44 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,nul
18/10/29 01:18:44 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
18/10/29 01:18:44 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
18/10/29 01:18:44 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}
18/10/29 01:18:44 INFO ui.SparkUI: Stopped Spark web UI at http://192.168.150.143:4040
18/10/29 01:18:44 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/10/29 01:18:44 INFO storage.MemoryStore: MemoryStore cleared
18/10/29 01:18:44 INFO storage.BlockManager: BlockManager stopped
18/10/29 01:18:44 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
18/10/29 01:18:44 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitC
18/10/29 01:18:44 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
18/10/29 01:18:44 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proce
18/10/29 01:18:44 INFO spark.SparkContext: Successfully stopped SparkContext
18/10/29 01:18:44 INFO util.ShutdownHookManager: Shutdown hook called
18/10/29 01:18:44 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-d3667600-902e-4dc7-9be9-
18/10/29 01:18:44 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-b59a7380-7c2b-4f2d-8cdd-
18/10/29 01:18:44 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-f2bc2c04-979f-4802-a064-
18/10/29 01:18:44 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
notroot@ubuntu:~$ jps
2511 JobHistoryServer
1567 NameNode
3575 Jps
2066 ResourceManager
1884 SecondaryNameNode
2179 NodeManager
1706 DataNode
notroot@ubuntu:~$ spark-s
spark-shell   spark-sql     spark-submit
notroot@ubuntu:~$ spark-shell
18/10/29 02:06:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform...
18/10/29 02:06:34 INFO spark.SecurityManager: Changing view acls to: notroot
18/10/29 02:06:34 INFO spark.SecurityManager: Changing modify acls to: notroot
18/10/29 02:06:34 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disableermissions: Set(notroot)
18/10/29 02:06:34 INFO spark.HttpServer: Starting HTTP Server
18/10/29 02:06:34 INFO server.Server: jetty-8.y.z-SNAPSHOT
18/10/29 02:06:34 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:50013
18/10/29 02:06:34 INFO util.Utils: Successfully started service 'HTTP class server' on port 50013.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.6.2
      /_/

Using Scala version 2.10.5 (OpenJDK 64-Bit Server VM, Java 1.7.0_79)
Type in expressions to have them evaluated.
Type :help for more information.
18/10/29 02:06:38 WARN util.Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; usi
18/10/29 02:06:38 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/10/29 02:06:38 INFO spark.SparkContext: Running Spark version 1.6.2
18/10/29 02:06:38 INFO spark.SecurityManager: Changing view acls to: notroot
18/10/29 02:06:38 INFO spark.SecurityManager: Changing modify acls to: notroot
18/10/29 02:06:38 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disableermissions: Set(notroot)
18/10/29 02:06:38 INFO util.Utils: Successfully started service 'sparkDriver' on port 38380.
18/10/29 02:06:39 INFO slf4j.Slf4jLogger: Slf4jLogger started
18/10/29 02:06:39 INFO Remoting: Starting remoting
18/10/29 02:06:39 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActor
18/10/29 02:06:39 INFO util.Utils: Successfully started service 'sparkDriverActorSystem' on port 58480.
18/10/29 02:06:39 INFO spark.SparkEnv: Registering MapOutputTracker
18/10/29 02:06:39 INFO spark.SparkEnv: Registering BlockManagerMaster
18/10/29 02:06:39 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-dd434af1-7379
18/10/29 02:06:39 INFO storage.MemoryStore: MemoryStore started with capacity 511.5 MB
18/10/29 02:06:39 INFO spark.SparkEnv: Registering OutputCommitCoordinator
18/10/29 02:06:39 INFO server.Server: jetty-8.y.z-SNAPSHOT
18/10/29 02:06:39 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040
18/10/29 02:06:39 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
18/10/29 02:06:39 INFO ui.SparkUI: Started SparkUI at http://192.168.150.143:4040
18/10/29 02:06:39 INFO executor.Executor: Starting executor ID driver on host localhost
18/10/29 02:06:39 INFO executor.Executor: Using REPL class URI: http://192.168.150.143:50013
18/10/29 02:06:39 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBl
18/10/29 02:06:39 INFO netty.NettyBlockTransferService: Server created on 59668
18/10/29 02:06:39 INFO storage.BlockManagerMaster: Trying to register BlockManager
18/10/29 02:06:39 INFO storage.BlockManagerMasterEndpoint: Registering block manager localhost:59668 wi
18/10/29 02:06:39 INFO storage.BlockManagerMaster: Registered BlockManager
18/10/29 02:06:40 INFO repl.SparkILoop: Created spark context..
Spark context available as sc.
18/10/29 02:06:40 INFO hive.HiveContext: Initializing execution hive, version 1.2.1
18/10/29 02:06:40 INFO client.ClientWrapper: Inspected Hadoop version: 2.6.0
18/10/29 02:06:40 INFO client.ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hado
18/10/29 02:06:41 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apach
18/10/29 02:06:41 INFO metastore.ObjectStore: ObjectStore, initialize called
18/10/29 02:06:41 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ign
18/10/29 02:06:41 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown -
18/10/29 02:06:41 WARN DataNucleus.Connection: BoneCP specified but not present in CLASSPATH (or one of
18/10/29 02:06:42 WARN DataNucleus.Connection: BoneCP specified but not present in CLASSPATH (or one of
18/10/29 02:06:43 INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.tabase,Type,FieldSchema,Order"
18/10/29 02:06:45 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSstore table.
18/10/29 02:06:45 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder"table.
18/10/29 02:06:46 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSstore table.
18/10/29 02:06:46 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder"table.
18/10/29 02:06:47 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
18/10/29 02:06:47 INFO metastore.ObjectStore: Initialized ObjectStore
18/10/29 02:06:47 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastorion 1.2.0
18/10/29 02:06:47 WARN metastore.ObjectStore: Failed to get database default, returning NoSuchObjectExc
18/10/29 02:06:47 INFO metastore.HiveMetaStore: Added admin role in metastore
18/10/29 02:06:47 INFO metastore.HiveMetaStore: Added public role in metastore
18/10/29 02:06:48 INFO metastore.HiveMetaStore: No user is added in admin role, since config is empty
18/10/29 02:06:48 INFO metastore.HiveMetaStore: 0: get_all_databases
18/10/29 02:06:48 INFO HiveMetaStore.audit: ugi=notroot ip=unknown-ip-addr      cmd=get_all_databases
18/10/29 02:06:48 INFO metastore.HiveMetaStore: 0: get_functions: db=default pat=*
18/10/29 02:06:48 INFO HiveMetaStore.audit: ugi=notroot ip=unknown-ip-addr      cmd=get_functions: db=d
18/10/29 02:06:48 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourstore table.
18/10/29 02:06:48 INFO session.SessionState: Created local directory: /tmp/064bc2c1-11e4-403c-9381-d3bc
18/10/29 02:06:48 INFO session.SessionState: Created HDFS directory: /tmp/hive/notroot/064bc2c1-11e4-40
18/10/29 02:06:48 INFO session.SessionState: Created local directory: /tmp/notroot/064bc2c1-11e4-403c-9
18/10/29 02:06:48 INFO session.SessionState: Created HDFS directory: /tmp/hive/notroot/064bc2c1-11e4-40
18/10/29 02:06:48 INFO hive.HiveContext: default warehouse location is /user/hive/warehouse
18/10/29 02:06:48 INFO hive.HiveContext: Initializing HiveMetastoreConnection version 1.2.1 using Spark
18/10/29 02:06:48 INFO client.ClientWrapper: Inspected Hadoop version: 2.6.0
18/10/29 02:06:48 INFO client.ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hado
18/10/29 02:06:49 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apach
18/10/29 02:06:49 INFO metastore.ObjectStore: ObjectStore, initialize called
18/10/29 02:06:50 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ign
18/10/29 02:06:50 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown -
18/10/29 02:06:50 WARN DataNucleus.Connection: BoneCP specified but not present in CLASSPATH (or one of
18/10/29 02:06:50 WARN DataNucleus.Connection: BoneCP specified but not present in CLASSPATH (or one of
18/10/29 02:06:51 INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.tabase,Type,FieldSchema,Order"
18/10/29 02:06:52 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSstore table.
18/10/29 02:06:52 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder"table.
18/10/29 02:06:53 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSstore table.
18/10/29 02:06:53 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder"table.
18/10/29 02:06:53 INFO DataNucleus.Query: Reading in results for query "org.datanucleus.store.rdbms.que
18/10/29 02:06:53 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
18/10/29 02:06:53 INFO metastore.ObjectStore: Initialized ObjectStore
18/10/29 02:06:53 INFO metastore.HiveMetaStore: Added admin role in metastore
18/10/29 02:06:53 INFO metastore.HiveMetaStore: Added public role in metastore
18/10/29 02:06:53 INFO metastore.HiveMetaStore: No user is added in admin role, since config is empty
18/10/29 02:06:53 INFO metastore.HiveMetaStore: 0: get_all_databases
18/10/29 02:06:53 INFO HiveMetaStore.audit: ugi=notroot ip=unknown-ip-addr      cmd=get_all_databases
18/10/29 02:06:53 INFO metastore.HiveMetaStore: 0: get_functions: db=default pat=*
18/10/29 02:06:53 INFO HiveMetaStore.audit: ugi=notroot ip=unknown-ip-addr      cmd=get_functions: db=d
18/10/29 02:06:53 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourstore table.
18/10/29 02:06:54 INFO session.SessionState: Created local directory: /tmp/18126f5f-38d9-4dd3-b791-ea90
18/10/29 02:06:54 INFO session.SessionState: Created HDFS directory: /tmp/hive/notroot/18126f5f-38d9-4d
18/10/29 02:06:54 INFO session.SessionState: Created local directory: /tmp/notroot/18126f5f-38d9-4dd3-b
18/10/29 02:06:54 INFO session.SessionState: Created HDFS directory: /tmp/hive/notroot/18126f5f-38d9-4d
18/10/29 02:06:54 INFO repl.SparkILoop: Created sql context (with Hive support)..
SQL context available as sqlContext.

scala> val fname = sc.textFile("file:///home/notroot/lab/data/sample")
18/10/29 02:10:10 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated siz
18/10/29 02:10:10 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimat
18/10/29 02:10:10 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:59668
18/10/29 02:10:10 INFO spark.SparkContext: Created broadcast 0 from textFile at <console>:27
fname: org.apache.spark.rdd.RDD[String] = file:///home/notroot/lab/data/sample MapPartitionsRDD[1] at t

scala> val fname1 = fname.flatMap(sou => sou.split(" "))
fname1: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at flatMap at <console>:29

scala> val fname2 = fname1.map(k => (k,1))
fname2: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[3] at map at <console>:31

scala> fname2.collect()
18/10/29 02:10:56 INFO mapred.FileInputFormat: Total input paths to process : 1
18/10/29 02:10:56 INFO spark.SparkContext: Starting job: collect at <console>:34
18/10/29 02:10:56 INFO scheduler.DAGScheduler: Got job 0 (collect at <console>:34) with 2 output partit
18/10/29 02:10:56 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (collect at <console>:34)
18/10/29 02:10:56 INFO scheduler.DAGScheduler: Parents of final stage: List()
18/10/29 02:10:56 INFO scheduler.DAGScheduler: Missing parents: List()
18/10/29 02:10:56 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at map at
18/10/29 02:10:56 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated siz
18/10/29 02:10:56 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimat
18/10/29 02:10:56 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:59668
18/10/29 02:10:56 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:100
18/10/29 02:10:56 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartit
18/10/29 02:10:56 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
18/10/29 02:10:56 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, part
18/10/29 02:10:56 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, part
18/10/29 02:10:56 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
18/10/29 02:10:56 INFO executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
18/10/29 02:10:56 INFO rdd.HadoopRDD: Input split: file:/home/notroot/lab/data/sample:18+19
18/10/29 02:10:56 INFO rdd.HadoopRDD: Input split: file:/home/notroot/lab/data/sample:0+18
18/10/29 02:10:56 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.t
18/10/29 02:10:56 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.
18/10/29 02:10:56 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use map
18/10/29 02:10:56 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.j
18/10/29 02:10:56 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapred
18/10/29 02:10:56 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2274 bytes result sen
18/10/29 02:10:56 INFO executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 2224 bytes result sen
18/10/29 02:10:56 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 123 ms on lo
18/10/29 02:10:56 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 101 ms on lo
18/10/29 02:10:56 INFO scheduler.DAGScheduler: ResultStage 0 (collect at <console>:34) finished in 0.13
18/10/29 02:10:56 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed
18/10/29 02:10:56 INFO scheduler.DAGScheduler: Job 0 finished: collect at <console>:34, took 0.206905 s
res0: Array[(String, Int)] = Array((How,1), (are,1), (you,1), (I,1), (am,1), (fine,1), (How,1), (about,

scala> val fname2 = fname1.map(k => (k,1))
fname2: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[4] at map at <console>:31

scala> val fname2 = fname1.map(k => (k,1))
fname2: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[5] at map at <console>:31

scala> val fname2 = fname1.map(k => (k,1))
fname2: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[6] at map at <console>:31

scala> val count = fname2.reduceByKey((k,a) => (k+a))
count: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[7] at reduceByKey at <console>:33

scala> count.collect()
18/10/29 02:25:11 INFO spark.SparkContext: Starting job: collect at <console>:36
18/10/29 02:25:11 INFO scheduler.DAGScheduler: Registering RDD 6 (map at <console>:31)
18/10/29 02:25:11 INFO scheduler.DAGScheduler: Got job 1 (collect at <console>:36) with 2 output partit
18/10/29 02:25:11 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (collect at <console>:36)
18/10/29 02:25:11 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
18/10/29 02:25:11 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 1)
18/10/29 02:25:11 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[6] at map
18/10/29 02:25:11 INFO storage.MemoryStore: Block broadcast_2 stored as values in memory (estimated siz
18/10/29 02:25:11 INFO storage.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimat
18/10/29 02:25:11 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:59668
18/10/29 02:25:11 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:100
18/10/29 02:25:11 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (MapPa
18/10/29 02:25:11 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
18/10/29 02:25:11 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, localhost, part
18/10/29 02:25:11 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, localhost, part
18/10/29 02:25:11 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 2)
18/10/29 02:25:11 INFO executor.Executor: Running task 1.0 in stage 1.0 (TID 3)
18/10/29 02:25:11 INFO rdd.HadoopRDD: Input split: file:/home/notroot/lab/data/sample:0+18
18/10/29 02:25:11 INFO rdd.HadoopRDD: Input split: file:/home/notroot/lab/data/sample:18+19
18/10/29 02:25:11 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 2). 2254 bytes result sen
18/10/29 02:25:11 INFO executor.Executor: Finished task 1.0 in stage 1.0 (TID 3). 2254 bytes result sen
18/10/29 02:25:11 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 76 ms on loc
18/10/29 02:25:11 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 77 ms on loc
18/10/29 02:25:11 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed
18/10/29 02:25:11 INFO scheduler.DAGScheduler: ShuffleMapStage 1 (map at <console>:31) finished in 0.08
18/10/29 02:25:11 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/10/29 02:25:11 INFO scheduler.DAGScheduler: running: Set()
18/10/29 02:25:11 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 2)
18/10/29 02:25:11 INFO scheduler.DAGScheduler: failed: Set()
18/10/29 02:25:11 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (ShuffledRDD[7] at reduceByKey
18/10/29 02:25:11 INFO storage.MemoryStore: Block broadcast_3 stored as values in memory (estimated siz
18/10/29 02:25:11 INFO storage.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimat
18/10/29 02:25:11 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:59668
18/10/29 02:25:11 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:100
18/10/29 02:25:11 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (ShuffledR
18/10/29 02:25:11 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 2 tasks
18/10/29 02:25:11 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, localhost, part
18/10/29 02:25:11 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, localhost, part
18/10/29 02:25:11 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 4)
18/10/29 02:25:11 INFO executor.Executor: Running task 1.0 in stage 2.0 (TID 5)
18/10/29 02:25:11 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
18/10/29 02:25:11 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
18/10/29 02:25:11 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
18/10/29 02:25:11 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
18/10/29 02:25:11 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 4). 1368 bytes result sen
18/10/29 02:25:11 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 40 ms on loc
18/10/29 02:25:11 INFO executor.Executor: Finished task 1.0 in stage 2.0 (TID 5). 1353 bytes result sen
18/10/29 02:25:11 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 46 ms on loc
18/10/29 02:25:11 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed
18/10/29 02:25:11 INFO scheduler.DAGScheduler: ResultStage 2 (collect at <console>:36) finished in 0.04
18/10/29 02:25:11 INFO scheduler.DAGScheduler: Job 1 finished: collect at <console>:36, took 0.170090 s
res1: Array[(String, Int)] = Array((are,1), (fine,1), (am,1), (How,2), (you,2), (about,1), (I,1))

scala> val count = fname2.reduceByKey(_ + _)
count: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[8] at reduceByKey at <console>:33

scala> count.collect()
18/10/29 02:25:36 INFO spark.SparkContext: Starting job: collect at <console>:36
18/10/29 02:25:36 INFO scheduler.DAGScheduler: Registering RDD 6 (map at <console>:31)
18/10/29 02:25:36 INFO scheduler.DAGScheduler: Got job 2 (collect at <console>:36) with 2 output partit
18/10/29 02:25:36 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at <console>:36)
18/10/29 02:25:36 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
18/10/29 02:25:36 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 3)
18/10/29 02:25:36 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[6] at map
18/10/29 02:25:36 INFO storage.MemoryStore: Block broadcast_4 stored as values in memory (estimated siz
18/10/29 02:25:36 INFO storage.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimat
18/10/29 02:25:36 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:59668
18/10/29 02:25:36 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:100
18/10/29 02:25:36 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 3 (MapPa
18/10/29 02:25:36 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 2 tasks
18/10/29 02:25:36 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 6, localhost, part
18/10/29 02:25:36 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 7, localhost, part
18/10/29 02:25:36 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 6)
18/10/29 02:25:36 INFO executor.Executor: Running task 1.0 in stage 3.0 (TID 7)
18/10/29 02:25:36 INFO rdd.HadoopRDD: Input split: file:/home/notroot/lab/data/sample:18+19
18/10/29 02:25:36 INFO rdd.HadoopRDD: Input split: file:/home/notroot/lab/data/sample:0+18
18/10/29 02:25:36 INFO executor.Executor: Finished task 1.0 in stage 3.0 (TID 7). 2254 bytes result sen
18/10/29 02:25:36 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 6). 2254 bytes result sen
18/10/29 02:25:36 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 7) in 35 ms on loc
18/10/29 02:25:36 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 6) in 37 ms on loc
18/10/29 02:25:36 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed
18/10/29 02:25:36 INFO scheduler.DAGScheduler: ShuffleMapStage 3 (map at <console>:31) finished in 0.02
18/10/29 02:25:36 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/10/29 02:25:36 INFO scheduler.DAGScheduler: running: Set()
18/10/29 02:25:36 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 4)
18/10/29 02:25:36 INFO scheduler.DAGScheduler: failed: Set()
18/10/29 02:25:36 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (ShuffledRDD[8] at reduceByKey
18/10/29 02:25:36 INFO storage.MemoryStore: Block broadcast_5 stored as values in memory (estimated siz
18/10/29 02:25:36 INFO storage.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimat
18/10/29 02:25:36 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:59668
18/10/29 02:25:36 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:100
18/10/29 02:25:36 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (ShuffledR
18/10/29 02:25:36 INFO scheduler.TaskSchedulerImpl: Adding task set 4.0 with 2 tasks
18/10/29 02:25:36 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 8, localhost, part
18/10/29 02:25:36 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 4.0 (TID 9, localhost, part
18/10/29 02:25:36 INFO executor.Executor: Running task 0.0 in stage 4.0 (TID 8)
18/10/29 02:25:36 INFO executor.Executor: Running task 1.0 in stage 4.0 (TID 9)
18/10/29 02:25:36 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
18/10/29 02:25:36 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
18/10/29 02:25:36 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
18/10/29 02:25:36 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
18/10/29 02:25:36 INFO executor.Executor: Finished task 1.0 in stage 4.0 (TID 9). 1353 bytes result sen
18/10/29 02:25:36 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 4.0 (TID 9) in 13 ms on loc
18/10/29 02:25:36 INFO executor.Executor: Finished task 0.0 in stage 4.0 (TID 8). 1368 bytes result sen
18/10/29 02:25:36 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 8) in 18 ms on loc
18/10/29 02:25:36 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed
18/10/29 02:25:36 INFO scheduler.DAGScheduler: ResultStage 4 (collect at <console>:36) finished in 0.02
18/10/29 02:25:36 INFO scheduler.DAGScheduler: Job 2 finished: collect at <console>:36, took 0.212126 s
res2: Array[(String, Int)] = Array((are,1), (fine,1), (am,1), (How,2), (you,2), (about,1), (I,1))

scala> val count = fname2.reduceByKey(_ + _)18/10/29 02:36:41 INFO storage.BlockManagerInfo: Removed bree: 511.5 MB)
18/10/29 02:36:41 INFO spark.ContextCleaner: Cleaned accumulator 5
18/10/29 02:36:41 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on localhost:59668 in memor
18/10/29 02:36:41 INFO spark.ContextCleaner: Cleaned accumulator 4
18/10/29 02:36:41 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on localhost:59668 in memor
18/10/29 02:36:41 INFO spark.ContextCleaner: Cleaned accumulator 3
18/10/29 02:36:41 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on localhost:59668 in memor
18/10/29 02:36:41 INFO spark.ContextCleaner: Cleaned accumulator 2
18/10/29 02:36:41 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on localhost:59668 in memor
18/10/29 02:36:41 INFO spark.ContextCleaner: Cleaned accumulator 1
count.saveAsTextFile(“file:///home/notroot/lab/data/results”)
<console>:1: error: ';' expected but '.' found.
       val count = fname2.reduceByKey(_ + _)count.saveAsTextFile(“file:///home/notroot/lab/data/results
                                                 ^
<console>:1: error: illegal character '\u201c'
       val count = fname2.reduceByKey(_ + _)count.saveAsTextFile(“file:///home/notroot/lab/data/results
                                                                 ^

scala> val count = fname2.reduceByKey(_ + _)count.saveAsTextFile("file:///home/notroot/lab/data/results
<console>:1: error: ';' expected but '.' found.
       val count = fname2.reduceByKey(_ + _)count.saveAsTextFile("file:///home/notroot/lab/data/results
                                                 ^

scala> val count = fname2.reduceByKey((k,a) => (k+a))
count: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[9] at reduceByKey at <console>:33

scala> count.collect()
18/10/29 02:38:19 INFO spark.SparkContext: Starting job: collect at <console>:36
18/10/29 02:38:19 INFO scheduler.DAGScheduler: Registering RDD 6 (map at <console>:31)
18/10/29 02:38:19 INFO scheduler.DAGScheduler: Got job 3 (collect at <console>:36) with 2 output partit
18/10/29 02:38:19 INFO scheduler.DAGScheduler: Final stage: ResultStage 6 (collect at <console>:36)
18/10/29 02:38:19 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
18/10/29 02:38:19 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 5)
18/10/29 02:38:19 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[6] at map
18/10/29 02:38:19 INFO storage.MemoryStore: Block broadcast_6 stored as values in memory (estimated siz
18/10/29 02:38:19 INFO storage.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimat
18/10/29 02:38:19 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:59668
18/10/29 02:38:19 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:100
18/10/29 02:38:19 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (MapPa
18/10/29 02:38:19 INFO scheduler.TaskSchedulerImpl: Adding task set 5.0 with 2 tasks
18/10/29 02:38:19 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 10, localhost, par
18/10/29 02:38:19 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 5.0 (TID 11, localhost, par
18/10/29 02:38:19 INFO executor.Executor: Running task 0.0 in stage 5.0 (TID 10)
18/10/29 02:38:19 INFO executor.Executor: Running task 1.0 in stage 5.0 (TID 11)
18/10/29 02:38:19 INFO rdd.HadoopRDD: Input split: file:/home/notroot/lab/data/sample:18+19
18/10/29 02:38:19 INFO rdd.HadoopRDD: Input split: file:/home/notroot/lab/data/sample:0+18
18/10/29 02:38:19 INFO executor.Executor: Finished task 1.0 in stage 5.0 (TID 11). 2254 bytes result se
18/10/29 02:38:19 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 5.0 (TID 11) in 16 ms on lo
18/10/29 02:38:19 INFO executor.Executor: Finished task 0.0 in stage 5.0 (TID 10). 2254 bytes result se
18/10/29 02:38:19 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 10) in 25 ms on lo
18/10/29 02:38:19 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed
18/10/29 02:38:19 INFO scheduler.DAGScheduler: ShuffleMapStage 5 (map at <console>:31) finished in 0.02
18/10/29 02:38:19 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/10/29 02:38:19 INFO scheduler.DAGScheduler: running: Set()
18/10/29 02:38:19 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 6)
18/10/29 02:38:19 INFO scheduler.DAGScheduler: failed: Set()
18/10/29 02:38:19 INFO scheduler.DAGScheduler: Submitting ResultStage 6 (ShuffledRDD[9] at reduceByKey
18/10/29 02:38:19 INFO storage.MemoryStore: Block broadcast_7 stored as values in memory (estimated siz
18/10/29 02:38:19 INFO storage.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimat
18/10/29 02:38:19 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:59668
18/10/29 02:38:19 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:100
18/10/29 02:38:19 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 6 (ShuffledR
18/10/29 02:38:19 INFO scheduler.TaskSchedulerImpl: Adding task set 6.0 with 2 tasks
18/10/29 02:38:19 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 12, localhost, par
18/10/29 02:38:19 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 13, localhost, par
18/10/29 02:38:19 INFO executor.Executor: Running task 1.0 in stage 6.0 (TID 13)
18/10/29 02:38:19 INFO executor.Executor: Running task 0.0 in stage 6.0 (TID 12)
18/10/29 02:38:19 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
18/10/29 02:38:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
18/10/29 02:38:19 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
18/10/29 02:38:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
18/10/29 02:38:19 INFO executor.Executor: Finished task 1.0 in stage 6.0 (TID 13). 1353 bytes result se
18/10/29 02:38:19 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 6.0 (TID 13) in 16 ms on lo
18/10/29 02:38:19 INFO executor.Executor: Finished task 0.0 in stage 6.0 (TID 12). 1368 bytes result se
18/10/29 02:38:19 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 12) in 22 ms on lo
18/10/29 02:38:19 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed
18/10/29 02:38:19 INFO scheduler.DAGScheduler: ResultStage 6 (collect at <console>:36) finished in 0.02
18/10/29 02:38:19 INFO scheduler.DAGScheduler: Job 3 finished: collect at <console>:36, took 0.063901 s
res3: Array[(String, Int)] = Array((are,1), (fine,1), (am,1), (How,2), (you,2), (about,1), (I,1))

scala> count.saveAsTextFile(“file:///home/notroot/lab/data/results”)
<console>:1: error: illegal character '\u201c'
       count.saveAsTextFile(“file:///home/notroot/lab/data/results”)
                            ^

scala> count.saveAsTextFile("file:///home/notroot/lab/data/results")
18/10/29 02:38:46 INFO spark.SparkContext: Starting job: saveAsTextFile at <console>:36
18/10/29 02:38:46 INFO spark.MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 155 bytes
18/10/29 02:38:46 INFO scheduler.DAGScheduler: Got job 4 (saveAsTextFile at <console>:36) with 2 output
18/10/29 02:38:46 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (saveAsTextFile at <console>:
18/10/29 02:38:46 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
18/10/29 02:38:46 INFO scheduler.DAGScheduler: Missing parents: List()
18/10/29 02:38:46 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[10] at saveAs
18/10/29 02:38:46 INFO storage.MemoryStore: Block broadcast_8 stored as values in memory (estimated siz
18/10/29 02:38:46 INFO storage.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimat
18/10/29 02:38:46 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:59668
18/10/29 02:38:46 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:100
18/10/29 02:38:46 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 8 (MapPartit
18/10/29 02:38:46 INFO scheduler.TaskSchedulerImpl: Adding task set 8.0 with 2 tasks
18/10/29 02:38:46 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 14, localhost, par
18/10/29 02:38:46 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 8.0 (TID 15, localhost, par
18/10/29 02:38:46 INFO executor.Executor: Running task 0.0 in stage 8.0 (TID 14)
18/10/29 02:38:46 INFO executor.Executor: Running task 1.0 in stage 8.0 (TID 15)
18/10/29 02:38:46 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
18/10/29 02:38:46 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
18/10/29 02:38:46 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
18/10/29 02:38:46 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
18/10/29 02:38:46 INFO output.FileOutputCommitter: Saved output of task 'attempt_201810290238_0008_m_00_201810290238_0008_m_000000
18/10/29 02:38:46 INFO mapred.SparkHadoopMapRedUtil: attempt_201810290238_0008_m_000000_14: Committed
18/10/29 02:38:46 INFO executor.Executor: Finished task 0.0 in stage 8.0 (TID 14). 2080 bytes result se
18/10/29 02:38:46 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 14) in 127 ms on l
18/10/29 02:38:46 INFO output.FileOutputCommitter: Saved output of task 'attempt_201810290238_0008_m_00_201810290238_0008_m_000001
18/10/29 02:38:46 INFO mapred.SparkHadoopMapRedUtil: attempt_201810290238_0008_m_000001_15: Committed
18/10/29 02:38:46 INFO executor.Executor: Finished task 1.0 in stage 8.0 (TID 15). 2080 bytes result se
18/10/29 02:38:46 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 8.0 (TID 15) in 131 ms on l
18/10/29 02:38:46 INFO scheduler.DAGScheduler: ResultStage 8 (saveAsTextFile at <console>:36) finished
18/10/29 02:38:46 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed
18/10/29 02:38:46 INFO scheduler.DAGScheduler: Job 4 finished: saveAsTextFile at <console>:36, took 0.2

scala> exit
warning: there were 1 deprecation warning(s); re-run with -deprecation for details
18/10/29 02:46:51 INFO spark.SparkContext: Invoking stop() from shutdown hook
18/10/29 02:46:51 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/static/sql,null}
18/10/29 02:46:51 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/SQL/execution/jso
18/10/29 02:46:51 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/SQL/execution,nul
18/10/29 02:46:51 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/SQL/json,null}
18/10/29 02:46:51 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/SQL,null}
18/10/29 02:46:51 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null
18/10/29 02:46:51 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill
18/10/29 02:46:51 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}
18/10/29 02:46:51 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
18/10/29 02:46:51 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
18/10/29 02:46:51 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadD
18/10/29 02:46:51 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadD
18/10/29 02:46:51 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,nu
18/10/29 02:46:51 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
18/10/29 02:46:51 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,
18/10/29 02:46:51 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
18/10/29 02:46:51 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,
18/10/29 02:46:51 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
18/10/29 02:46:51 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null
18/10/29 02:46:51 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}
18/10/29 02:46:51 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,
18/10/29 02:46:51 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
18/10/29 02:46:51 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json
18/10/29 02:46:51 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null
18/10/29 02:46:51 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
18/10/29 02:46:51 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}
18/10/29 02:46:51 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,nul
18/10/29 02:46:51 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
18/10/29 02:46:51 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
18/10/29 02:46:51 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}
18/10/29 02:46:51 INFO ui.SparkUI: Stopped Spark web UI at http://192.168.150.143:4040
18/10/29 02:46:51 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/10/29 02:46:51 INFO storage.MemoryStore: MemoryStore cleared
18/10/29 02:46:51 INFO storage.BlockManager: BlockManager stopped
18/10/29 02:46:51 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
18/10/29 02:46:51 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitC
18/10/29 02:46:51 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
18/10/29 02:46:51 INFO spark.SparkContext: Successfully stopped SparkContext
18/10/29 02:46:51 INFO util.ShutdownHookManager: Shutdown hook called
18/10/29 02:46:51 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-baa45393-2c53-40a2-9ab9-
18/10/29 02:46:51 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-5ba42ae0-b04e-4e9c-8575-
18/10/29 02:46:51 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proce
18/10/29 02:46:51 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-2172bce6-2b98-42ff-839d-
18/10/29 02:46:51 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
notroot@ubuntu:~$ pyspark
Python 2.7.6 (default, Jun 22 2015, 17:58:13)
[GCC 4.8.2] on linux2
Type "help", "copyright", "credits" or "license" for more information.
18/10/29 02:46:56 INFO spark.SparkContext: Running Spark version 1.6.2
18/10/29 02:46:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform...
18/10/29 02:46:56 WARN util.Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; usi
18/10/29 02:46:56 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/10/29 02:46:56 INFO spark.SecurityManager: Changing view acls to: notroot
18/10/29 02:46:56 INFO spark.SecurityManager: Changing modify acls to: notroot
18/10/29 02:46:56 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disableermissions: Set(notroot)
18/10/29 02:46:57 INFO util.Utils: Successfully started service 'sparkDriver' on port 57792.
18/10/29 02:46:57 INFO slf4j.Slf4jLogger: Slf4jLogger started
18/10/29 02:46:57 INFO Remoting: Starting remoting
18/10/29 02:46:57 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActor
18/10/29 02:46:57 INFO util.Utils: Successfully started service 'sparkDriverActorSystem' on port 53638.
18/10/29 02:46:57 INFO spark.SparkEnv: Registering MapOutputTracker
18/10/29 02:46:57 INFO spark.SparkEnv: Registering BlockManagerMaster
18/10/29 02:46:57 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-96976e48-9cfd
18/10/29 02:46:57 INFO storage.MemoryStore: MemoryStore started with capacity 511.5 MB
18/10/29 02:46:57 INFO spark.SparkEnv: Registering OutputCommitCoordinator
18/10/29 02:46:58 INFO server.Server: jetty-8.y.z-SNAPSHOT
18/10/29 02:46:58 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040
18/10/29 02:46:58 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
18/10/29 02:46:58 INFO ui.SparkUI: Started SparkUI at http://192.168.150.143:4040
18/10/29 02:46:58 INFO executor.Executor: Starting executor ID driver on host localhost
18/10/29 02:46:58 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBl
18/10/29 02:46:58 INFO netty.NettyBlockTransferService: Server created on 39380
18/10/29 02:46:58 INFO storage.BlockManagerMaster: Trying to register BlockManager
18/10/29 02:46:58 INFO storage.BlockManagerMasterEndpoint: Registering block manager localhost:39380 wi
18/10/29 02:46:58 INFO storage.BlockManagerMaster: Registered BlockManager
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 1.6.2
      /_/

Using Python version 2.7.6 (default, Jun 22 2015 17:58:13)
SparkContext available as sc, HiveContext available as sqlContext.
>>> jps
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'jps' is not defined
>>> sc
<pyspark.context.SparkContext object at 0x7f1e396ca0d0>
>>> sc.appName
u'PySparkShell'
>>> sc.master
u'local[*]'
>>> baseRdd=sc.textFile("file:///home/notroot/lab/data/sample")
18/10/29 02:48:18 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated siz
18/10/29 02:48:18 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimat
18/10/29 02:48:18 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:39380
18/10/29 02:48:18 INFO spark.SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImp
>>> baseRdd.take(1)
18/10/29 02:48:48 INFO mapred.FileInputFormat: Total input paths to process : 1
18/10/29 02:48:48 INFO spark.SparkContext: Starting job: runJob at PythonRDD.scala:393
18/10/29 02:48:48 INFO scheduler.DAGScheduler: Got job 0 (runJob at PythonRDD.scala:393) with 1 output
18/10/29 02:48:48 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (runJob at PythonRDD.scala:39
18/10/29 02:48:48 INFO scheduler.DAGScheduler: Parents of final stage: List()
18/10/29 02:48:48 INFO scheduler.DAGScheduler: Missing parents: List()
18/10/29 02:48:48 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at RDD at PythonR
18/10/29 02:48:48 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated siz
18/10/29 02:48:48 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimat
18/10/29 02:48:48 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:39380
18/10/29 02:48:48 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:100
18/10/29 02:48:48 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD
18/10/29 02:48:48 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
18/10/29 02:48:48 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, part
18/10/29 02:48:48 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
18/10/29 02:48:48 INFO rdd.HadoopRDD: Input split: file:/home/notroot/lab/data/sample:0+18
18/10/29 02:48:48 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.t
18/10/29 02:48:48 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.
18/10/29 02:48:48 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapred
18/10/29 02:48:48 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use map
18/10/29 02:48:48 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.j
18/10/29 02:48:49 INFO python.PythonRunner: Times: total = 209, boot = 189, init = 20, finish = 0
18/10/29 02:48:49 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2140 bytes result sen
18/10/29 02:48:49 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 307 ms on lo
18/10/29 02:48:49 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed
18/10/29 02:48:49 INFO scheduler.DAGScheduler: ResultStage 0 (runJob at PythonRDD.scala:393) finished i
18/10/29 02:48:49 INFO scheduler.DAGScheduler: Job 0 finished: runJob at PythonRDD.scala:393, took 0.45
[u'How are you']
>>> baseRdd.take(2)
18/10/29 02:49:24 INFO spark.SparkContext: Starting job: runJob at PythonRDD.scala:393
18/10/29 02:49:24 INFO scheduler.DAGScheduler: Got job 1 (runJob at PythonRDD.scala:393) with 1 output
18/10/29 02:49:24 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (runJob at PythonRDD.scala:39
18/10/29 02:49:24 INFO scheduler.DAGScheduler: Parents of final stage: List()
18/10/29 02:49:24 INFO scheduler.DAGScheduler: Missing parents: List()
18/10/29 02:49:24 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (PythonRDD[3] at RDD at PythonR
18/10/29 02:49:24 INFO storage.MemoryStore: Block broadcast_2 stored as values in memory (estimated siz
18/10/29 02:49:24 INFO storage.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimat
18/10/29 02:49:24 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:39380
18/10/29 02:49:24 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:100
18/10/29 02:49:24 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (PythonRDD
18/10/29 02:49:24 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
18/10/29 02:49:24 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, part
18/10/29 02:49:24 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)
18/10/29 02:49:24 INFO rdd.HadoopRDD: Input split: file:/home/notroot/lab/data/sample:0+18
18/10/29 02:49:24 INFO python.PythonRunner: Times: total = 24, boot = 3, init = 21, finish = 0
18/10/29 02:49:24 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 2173 bytes result sen
18/10/29 02:49:24 INFO scheduler.DAGScheduler: ResultStage 1 (runJob at PythonRDD.scala:393) finished i
18/10/29 02:49:24 INFO scheduler.DAGScheduler: Job 1 finished: runJob at PythonRDD.scala:393, took 0.07
18/10/29 02:49:24 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 55 ms on loc
18/10/29 02:49:24 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed
[u'How are you', u'I am fine']
>>> baseRdd.take(3)
18/10/29 02:49:28 INFO spark.SparkContext: Starting job: runJob at PythonRDD.scala:393
18/10/29 02:49:28 INFO scheduler.DAGScheduler: Got job 2 (runJob at PythonRDD.scala:393) with 1 output
18/10/29 02:49:28 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (runJob at PythonRDD.scala:39
18/10/29 02:49:28 INFO scheduler.DAGScheduler: Parents of final stage: List()
18/10/29 02:49:28 INFO scheduler.DAGScheduler: Missing parents: List()
18/10/29 02:49:28 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (PythonRDD[4] at RDD at PythonR
18/10/29 02:49:28 INFO storage.MemoryStore: Block broadcast_3 stored as values in memory (estimated siz
18/10/29 02:49:28 INFO storage.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimat
18/10/29 02:49:28 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:39380
18/10/29 02:49:28 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:100
18/10/29 02:49:28 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (PythonRDD
18/10/29 02:49:28 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
18/10/29 02:49:28 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, part
18/10/29 02:49:28 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 2)
18/10/29 02:49:28 INFO rdd.HadoopRDD: Input split: file:/home/notroot/lab/data/sample:0+18
18/10/29 02:49:28 INFO python.PythonRunner: Times: total = 12, boot = 2, init = 10, finish = 0
18/10/29 02:49:28 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 2). 2173 bytes result sen
18/10/29 02:49:28 INFO scheduler.DAGScheduler: ResultStage 2 (runJob at PythonRDD.scala:393) finished i
18/10/29 02:49:28 INFO scheduler.DAGScheduler: Job 2 finished: runJob at PythonRDD.scala:393, took 0.04
18/10/29 02:49:28 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 36 ms on loc
18/10/29 02:49:28 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed
18/10/29 02:49:28 INFO spark.SparkContext: Starting job: runJob at PythonRDD.scala:393
18/10/29 02:49:28 INFO scheduler.DAGScheduler: Got job 3 (runJob at PythonRDD.scala:393) with 1 output
18/10/29 02:49:28 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (runJob at PythonRDD.scala:39
18/10/29 02:49:28 INFO scheduler.DAGScheduler: Parents of final stage: List()
18/10/29 02:49:28 INFO scheduler.DAGScheduler: Missing parents: List()
18/10/29 02:49:28 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (PythonRDD[5] at RDD at PythonR
18/10/29 02:49:28 INFO storage.MemoryStore: Block broadcast_4 stored as values in memory (estimated siz
18/10/29 02:49:28 INFO storage.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimat
18/10/29 02:49:28 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:39380
18/10/29 02:49:28 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:100
18/10/29 02:49:28 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (PythonRDD
18/10/29 02:49:28 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
18/10/29 02:49:28 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, part
18/10/29 02:49:28 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 3)
18/10/29 02:49:28 INFO rdd.HadoopRDD: Input split: file:/home/notroot/lab/data/sample:18+19
18/10/29 02:49:28 INFO python.PythonRunner: Times: total = 38, boot = -51, init = 89, finish = 0
18/10/29 02:49:28 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 3). 2142 bytes result sen
18/10/29 02:49:28 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 53 ms on loc
18/10/29 02:49:28 INFO scheduler.DAGScheduler: ResultStage 3 (runJob at PythonRDD.scala:393) finished i
18/10/29 02:49:28 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed
18/10/29 02:49:28 INFO scheduler.DAGScheduler: Job 3 finished: runJob at PythonRDD.scala:393, took 0.07
[u'How are you', u'I am fine', u'How about you']
>>> splitRdd = baseRdd.flatMap(lambda line: line.split(" "))
>>> splitRdd.take(5)
18/10/29 02:50:20 INFO spark.SparkContext: Starting job: runJob at PythonRDD.scala:393
18/10/29 02:50:20 INFO scheduler.DAGScheduler: Got job 4 (runJob at PythonRDD.scala:393) with 1 output
18/10/29 02:50:20 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (runJob at PythonRDD.scala:39
18/10/29 02:50:20 INFO scheduler.DAGScheduler: Parents of final stage: List()
18/10/29 02:50:20 INFO scheduler.DAGScheduler: Missing parents: List()
18/10/29 02:50:20 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (PythonRDD[6] at RDD at PythonR
18/10/29 02:50:20 INFO storage.MemoryStore: Block broadcast_5 stored as values in memory (estimated siz
18/10/29 02:50:20 INFO storage.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimat
18/10/29 02:50:20 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:39380
18/10/29 02:50:20 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:100
18/10/29 02:50:20 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (PythonRDD
18/10/29 02:50:20 INFO scheduler.TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
18/10/29 02:50:20 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, part
18/10/29 02:50:20 INFO executor.Executor: Running task 0.0 in stage 4.0 (TID 4)
18/10/29 02:50:20 INFO rdd.HadoopRDD: Input split: file:/home/notroot/lab/data/sample:0+18
18/10/29 02:50:20 INFO python.PythonRunner: Times: total = 12, boot = 5, init = 7, finish = 0
18/10/29 02:50:20 INFO executor.Executor: Finished task 0.0 in stage 4.0 (TID 4). 2203 bytes result sen
18/10/29 02:50:20 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 30 ms on loc
18/10/29 02:50:20 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed
18/10/29 02:50:20 INFO scheduler.DAGScheduler: ResultStage 4 (runJob at PythonRDD.scala:393) finished i
18/10/29 02:50:20 INFO scheduler.DAGScheduler: Job 4 finished: runJob at PythonRDD.scala:393, took 0.06
[u'How', u'are', u'you', u'I', u'am']
>>>
>>>
>>> print(splitRdd)
PythonRDD[7] at RDD at PythonRDD.scala:43
>>> mappedRdd = splitRdd.map(lambda line: (line,1))
>>> mappedRdd.take(5)
18/10/29 02:51:57 INFO spark.SparkContext: Starting job: runJob at PythonRDD.scala:393
18/10/29 02:51:57 INFO scheduler.DAGScheduler: Got job 5 (runJob at PythonRDD.scala:393) with 1 output
18/10/29 02:51:57 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (runJob at PythonRDD.scala:39
18/10/29 02:51:57 INFO scheduler.DAGScheduler: Parents of final stage: List()
18/10/29 02:51:57 INFO scheduler.DAGScheduler: Missing parents: List()
18/10/29 02:51:57 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (PythonRDD[8] at RDD at PythonR
18/10/29 02:51:57 INFO storage.MemoryStore: Block broadcast_6 stored as values in memory (estimated siz
18/10/29 02:51:57 INFO storage.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimat
18/10/29 02:51:57 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:39380
18/10/29 02:51:57 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:100
18/10/29 02:51:57 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (PythonRDD
18/10/29 02:51:57 INFO scheduler.TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
18/10/29 02:51:57 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, part
18/10/29 02:51:57 INFO executor.Executor: Running task 0.0 in stage 5.0 (TID 5)
18/10/29 02:51:57 INFO rdd.HadoopRDD: Input split: file:/home/notroot/lab/data/sample:0+18
18/10/29 02:51:57 INFO python.PythonRunner: Times: total = 6, boot = 2, init = 3, finish = 1
18/10/29 02:51:57 INFO executor.Executor: Finished task 0.0 in stage 5.0 (TID 5). 2226 bytes result sen
18/10/29 02:51:57 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 32 ms on loc
18/10/29 02:51:57 INFO scheduler.DAGScheduler: ResultStage 5 (runJob at PythonRDD.scala:393) finished i
18/10/29 02:51:57 INFO scheduler.DAGScheduler: Job 5 finished: runJob at PythonRDD.scala:393, took 0.04
18/10/29 02:51:57 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed
[(u'How', 1), (u'are', 1), (u'you', 1), (u'I', 1), (u'am', 1)]
>>> reducedRdd = mappedRdd.reduceByKey(lambda a,b: a+b)
>>> reducedRdd.take(20)
18/10/29 02:52:28 INFO spark.SparkContext: Starting job: runJob at PythonRDD.scala:393
18/10/29 02:52:28 INFO scheduler.DAGScheduler: Registering RDD 10 (reduceByKey at <stdin>:1)
18/10/29 02:52:28 INFO scheduler.DAGScheduler: Got job 6 (runJob at PythonRDD.scala:393) with 1 output
18/10/29 02:52:28 INFO scheduler.DAGScheduler: Final stage: ResultStage 7 (runJob at PythonRDD.scala:39
18/10/29 02:52:28 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
18/10/29 02:52:28 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 6)
18/10/29 02:52:28 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (PairwiseRDD[10] at reduceB
18/10/29 02:52:28 INFO storage.MemoryStore: Block broadcast_7 stored as values in memory (estimated siz
18/10/29 02:52:28 INFO storage.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimat
18/10/29 02:52:28 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:39380
18/10/29 02:52:28 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:100
18/10/29 02:52:28 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (Pairw
18/10/29 02:52:28 INFO scheduler.TaskSchedulerImpl: Adding task set 6.0 with 2 tasks
18/10/29 02:52:28 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6, localhost, part
18/10/29 02:52:28 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 7, localhost, part
18/10/29 02:52:28 INFO executor.Executor: Running task 0.0 in stage 6.0 (TID 6)
18/10/29 02:52:28 INFO executor.Executor: Running task 1.0 in stage 6.0 (TID 7)
18/10/29 02:52:28 INFO rdd.HadoopRDD: Input split: file:/home/notroot/lab/data/sample:0+18
18/10/29 02:52:28 INFO rdd.HadoopRDD: Input split: file:/home/notroot/lab/data/sample:18+19
18/10/29 02:52:28 INFO python.PythonRunner: Times: total = 19, boot = 1, init = 15, finish = 3
18/10/29 02:52:28 INFO python.PythonRunner: Times: total = 18, boot = 1, init = 14, finish = 3
18/10/29 02:52:28 INFO executor.Executor: Finished task 1.0 in stage 6.0 (TID 7). 2318 bytes result sen
18/10/29 02:52:28 INFO executor.Executor: Finished task 0.0 in stage 6.0 (TID 6). 2318 bytes result sen
18/10/29 02:52:28 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 6.0 (TID 7) in 84 ms on loc
18/10/29 02:52:28 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 89 ms on loc
18/10/29 02:52:28 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed
18/10/29 02:52:28 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (reduceByKey at <stdin>:1) finished in
18/10/29 02:52:28 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/10/29 02:52:28 INFO scheduler.DAGScheduler: running: Set()
18/10/29 02:52:28 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 7)
18/10/29 02:52:28 INFO scheduler.DAGScheduler: failed: Set()
18/10/29 02:52:28 INFO scheduler.DAGScheduler: Submitting ResultStage 7 (PythonRDD[13] at RDD at Python
18/10/29 02:52:28 INFO storage.MemoryStore: Block broadcast_8 stored as values in memory (estimated siz
18/10/29 02:52:28 INFO storage.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimat
18/10/29 02:52:28 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:39380
18/10/29 02:52:28 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:100
18/10/29 02:52:28 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (PythonRDD
18/10/29 02:52:28 INFO scheduler.TaskSchedulerImpl: Adding task set 7.0 with 1 tasks
18/10/29 02:52:28 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 8, localhost, part
18/10/29 02:52:28 INFO executor.Executor: Running task 0.0 in stage 7.0 (TID 8)
18/10/29 02:52:28 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
18/10/29 02:52:28 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
18/10/29 02:52:28 INFO python.PythonRunner: Times: total = 41, boot = -83, init = 124, finish = 0
18/10/29 02:52:28 INFO executor.Executor: Finished task 0.0 in stage 7.0 (TID 8). 1344 bytes result sen
18/10/29 02:52:28 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 8) in 74 ms on loc
18/10/29 02:52:28 INFO scheduler.DAGScheduler: ResultStage 7 (runJob at PythonRDD.scala:393) finished i
18/10/29 02:52:28 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed
18/10/29 02:52:28 INFO scheduler.DAGScheduler: Job 6 finished: runJob at PythonRDD.scala:393, took 0.21
18/10/29 02:52:28 INFO spark.SparkContext: Starting job: runJob at PythonRDD.scala:393
18/10/29 02:52:28 INFO spark.MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 155 bytes
18/10/29 02:52:28 INFO scheduler.DAGScheduler: Got job 7 (runJob at PythonRDD.scala:393) with 1 output
18/10/29 02:52:28 INFO scheduler.DAGScheduler: Final stage: ResultStage 9 (runJob at PythonRDD.scala:39
18/10/29 02:52:28 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)
18/10/29 02:52:28 INFO scheduler.DAGScheduler: Missing parents: List()
18/10/29 02:52:28 INFO scheduler.DAGScheduler: Submitting ResultStage 9 (PythonRDD[14] at RDD at Python
18/10/29 02:52:28 INFO storage.MemoryStore: Block broadcast_9 stored as values in memory (estimated siz
18/10/29 02:52:28 INFO storage.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimat
18/10/29 02:52:28 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:39380
18/10/29 02:52:28 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:100
18/10/29 02:52:28 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (PythonRDD
18/10/29 02:52:28 INFO scheduler.TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
18/10/29 02:52:28 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9, localhost, part
18/10/29 02:52:28 INFO executor.Executor: Running task 0.0 in stage 9.0 (TID 9)
18/10/29 02:52:28 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
18/10/29 02:52:28 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
18/10/29 02:52:28 INFO python.PythonRunner: Times: total = 38, boot = -163, init = 201, finish = 0
18/10/29 02:52:28 INFO executor.Executor: Finished task 0.0 in stage 9.0 (TID 9). 1286 bytes result sen
18/10/29 02:52:28 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 52 ms on loc
18/10/29 02:52:28 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed
18/10/29 02:52:28 INFO scheduler.DAGScheduler: ResultStage 9 (runJob at PythonRDD.scala:393) finished i
18/10/29 02:52:28 INFO scheduler.DAGScheduler: Job 7 finished: runJob at PythonRDD.scala:393, took 0.07
[(u'I', 1), (u'fine', 1), (u'you', 2), (u'am', 1), (u'about', 1), (u'How', 2), (u'are', 1)]
>>> reducedRdd.collect()
18/10/29 02:52:42 INFO spark.SparkContext: Starting job: collect at <stdin>:1
18/10/29 02:52:42 INFO scheduler.DAGScheduler: Got job 8 (collect at <stdin>:1) with 2 output partition
18/10/29 02:52:42 INFO scheduler.DAGScheduler: Final stage: ResultStage 11 (collect at <stdin>:1)
18/10/29 02:52:42 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
18/10/29 02:52:42 INFO scheduler.DAGScheduler: Missing parents: List()
18/10/29 02:52:42 INFO scheduler.DAGScheduler: Submitting ResultStage 11 (PythonRDD[15] at collect at <
18/10/29 02:52:42 INFO storage.MemoryStore: Block broadcast_10 stored as values in memory (estimated si
18/10/29 02:52:42 INFO storage.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estima
18/10/29 02:52:42 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:39380
18/10/29 02:52:42 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:10
18/10/29 02:52:42 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 11 (PythonRD
18/10/29 02:52:42 INFO scheduler.TaskSchedulerImpl: Adding task set 11.0 with 2 tasks
18/10/29 02:52:42 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 10, localhost, pa
18/10/29 02:52:42 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 11.0 (TID 11, localhost, pa
18/10/29 02:52:42 INFO executor.Executor: Running task 0.0 in stage 11.0 (TID 10)
18/10/29 02:52:42 INFO executor.Executor: Running task 1.0 in stage 11.0 (TID 11)
18/10/29 02:52:42 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
18/10/29 02:52:42 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
18/10/29 02:52:42 INFO storage.ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
18/10/29 02:52:42 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
18/10/29 02:52:42 INFO python.PythonRunner: Times: total = 41, boot = -14281, init = 14321, finish = 1
18/10/29 02:52:42 INFO executor.Executor: Finished task 1.0 in stage 11.0 (TID 11). 1286 bytes result s
18/10/29 02:52:42 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 11.0 (TID 11) in 55 ms on l
18/10/29 02:52:42 INFO python.PythonRunner: Times: total = 41, boot = -14204, init = 14244, finish = 1
18/10/29 02:52:42 INFO executor.Executor: Finished task 0.0 in stage 11.0 (TID 10). 1344 bytes result s
18/10/29 02:52:42 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 10) in 67 ms on l
18/10/29 02:52:42 INFO scheduler.DAGScheduler: ResultStage 11 (collect at <stdin>:1) finished in 0.062
18/10/29 02:52:42 INFO scheduler.DAGScheduler: Job 8 finished: collect at <stdin>:1, took 0.087839 s
18/10/29 02:52:42 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all complete
[(u'I', 1), (u'fine', 1), (u'you', 2), (u'am', 1), (u'about', 1), (u'How', 2), (u'are', 1)]
>>> sc.master
u'local[*]'
>>> baseRdd=sc.textFile("file:///home/notroot/lab/data/sample")
18/10/29 03:07:21 INFO storage.MemoryStore: Block broadcast_11 stored as values in memory (estimated si
18/10/29 03:07:21 INFO storage.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estima
18/10/29 03:07:21 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:39380
18/10/29 03:07:21 INFO spark.SparkContext: Created broadcast 11 from textFile at NativeMethodAccessorIm
>>> baseRdd.take(3)
18/10/29 03:07:35 INFO mapred.FileInputFormat: Total input paths to process : 1
18/10/29 03:07:35 INFO spark.SparkContext: Starting job: runJob at PythonRDD.scala:393
18/10/29 03:07:35 INFO scheduler.DAGScheduler: Got job 9 (runJob at PythonRDD.scala:393) with 1 output
18/10/29 03:07:35 INFO scheduler.DAGScheduler: Final stage: ResultStage 12 (runJob at PythonRDD.scala:3
18/10/29 03:07:35 INFO scheduler.DAGScheduler: Parents of final stage: List()
18/10/29 03:07:35 INFO scheduler.DAGScheduler: Missing parents: List()
18/10/29 03:07:35 INFO scheduler.DAGScheduler: Submitting ResultStage 12 (PythonRDD[18] at RDD at Pytho
18/10/29 03:07:35 INFO storage.MemoryStore: Block broadcast_12 stored as values in memory (estimated si
18/10/29 03:07:35 INFO storage.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estima
18/10/29 03:07:35 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:39380
18/10/29 03:07:35 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:10
18/10/29 03:07:35 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (PythonRD
18/10/29 03:07:35 INFO scheduler.TaskSchedulerImpl: Adding task set 12.0 with 1 tasks
18/10/29 03:07:35 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12, localhost, pa
18/10/29 03:07:35 INFO executor.Executor: Running task 0.0 in stage 12.0 (TID 12)
18/10/29 03:07:35 INFO rdd.HadoopRDD: Input split: file:/home/notroot/lab/data/sample:0+18
18/10/29 03:07:35 INFO python.PythonRunner: Times: total = 9, boot = 7, init = 1, finish = 1
18/10/29 03:07:35 INFO executor.Executor: Finished task 0.0 in stage 12.0 (TID 12). 2173 bytes result s
18/10/29 03:07:35 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 36 ms on l
18/10/29 03:07:35 INFO scheduler.DAGScheduler: ResultStage 12 (runJob at PythonRDD.scala:393) finished
18/10/29 03:07:35 INFO scheduler.DAGScheduler: Job 9 finished: runJob at PythonRDD.scala:393, took 0.06
18/10/29 03:07:35 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all complete
18/10/29 03:07:35 INFO spark.SparkContext: Starting job: runJob at PythonRDD.scala:393
18/10/29 03:07:35 INFO scheduler.DAGScheduler: Got job 10 (runJob at PythonRDD.scala:393) with 1 output
18/10/29 03:07:35 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (runJob at PythonRDD.scala:3
18/10/29 03:07:35 INFO scheduler.DAGScheduler: Parents of final stage: List()
18/10/29 03:07:35 INFO scheduler.DAGScheduler: Missing parents: List()
18/10/29 03:07:35 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (PythonRDD[19] at RDD at Pytho
18/10/29 03:07:35 INFO storage.MemoryStore: Block broadcast_13 stored as values in memory (estimated si
18/10/29 03:07:35 INFO storage.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estima
18/10/29 03:07:35 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on localhost:39380
18/10/29 03:07:35 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:10
18/10/29 03:07:35 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (PythonRD
18/10/29 03:07:35 INFO scheduler.TaskSchedulerImpl: Adding task set 13.0 with 1 tasks
18/10/29 03:07:35 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13, localhost, pa
18/10/29 03:07:35 INFO executor.Executor: Running task 0.0 in stage 13.0 (TID 13)
18/10/29 03:07:35 INFO rdd.HadoopRDD: Input split: file:/home/notroot/lab/data/sample:18+19
18/10/29 03:07:35 INFO python.PythonRunner: Times: total = 38, boot = -34, init = 72, finish = 0
18/10/29 03:07:35 INFO executor.Executor: Finished task 0.0 in stage 13.0 (TID 13). 2142 bytes result s
18/10/29 03:07:35 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 51 ms on l
18/10/29 03:07:35 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all complete
18/10/29 03:07:35 INFO scheduler.DAGScheduler: ResultStage 13 (runJob at PythonRDD.scala:393) finished
18/10/29 03:07:35 INFO scheduler.DAGScheduler: Job 10 finished: runJob at PythonRDD.scala:393, took 0.0
[u'How are you', u'I am fine', u'How about you']
>>> 18/10/29 03:07:46 INFO spark.ContextCleaner: Cleaned accumulator 2
18/10/29 03:07:46 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on localhost:39380 in memor
18/10/29 03:07:46 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on localhost:39380 in memo
18/10/29 03:07:46 INFO spark.ContextCleaner: Cleaned accumulator 13
18/10/29 03:07:46 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on localhost:39380 in memo
18/10/29 03:07:46 INFO spark.ContextCleaner: Cleaned accumulator 12
18/10/29 03:07:46 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on localhost:39380 in memo
18/10/29 03:07:46 INFO spark.ContextCleaner: Cleaned accumulator 11
18/10/29 03:07:46 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on localhost:39380 in memor
18/10/29 03:07:46 INFO spark.ContextCleaner: Cleaned accumulator 10
18/10/29 03:07:46 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on localhost:39380 in memor
18/10/29 03:07:46 INFO spark.ContextCleaner: Cleaned accumulator 9
18/10/29 03:07:46 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on localhost:39380 in memor
18/10/29 03:07:46 INFO spark.ContextCleaner: Cleaned accumulator 8
18/10/29 03:07:46 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on localhost:39380 in memor
18/10/29 03:07:46 INFO spark.ContextCleaner: Cleaned accumulator 7
18/10/29 03:07:46 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on localhost:39380 in memor
18/10/29 03:07:46 INFO spark.ContextCleaner: Cleaned accumulator 6
18/10/29 03:07:46 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on localhost:39380 in memor
18/10/29 03:07:46 INFO spark.ContextCleaner: Cleaned accumulator 5
18/10/29 03:07:46 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on localhost:39380 in memor
18/10/29 03:07:46 INFO spark.ContextCleaner: Cleaned accumulator 4
18/10/29 03:07:46 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on localhost:39380 in memor
18/10/29 03:07:46 INFO spark.ContextCleaner: Cleaned accumulator 3

>>> exit
Use exit() or Ctrl-D (i.e. EOF) to exit
>>> exit()
18/10/29 03:54:13 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null
18/10/29 03:54:13 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill
18/10/29 03:54:13 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}
18/10/29 03:54:13 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
18/10/29 03:54:13 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
18/10/29 03:54:13 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadD
18/10/29 03:54:13 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadD
18/10/29 03:54:13 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,nu
18/10/29 03:54:13 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
18/10/29 03:54:13 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,
18/10/29 03:54:13 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
18/10/29 03:54:13 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,
18/10/29 03:54:13 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
18/10/29 03:54:13 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null
18/10/29 03:54:13 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}
18/10/29 03:54:13 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,
18/10/29 03:54:13 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
18/10/29 03:54:13 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json
18/10/29 03:54:13 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null
18/10/29 03:54:13 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
18/10/29 03:54:13 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}
18/10/29 03:54:13 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,nul
18/10/29 03:54:13 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
18/10/29 03:54:13 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
18/10/29 03:54:13 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}
18/10/29 03:54:13 INFO ui.SparkUI: Stopped Spark web UI at http://192.168.150.143:4040
18/10/29 03:54:13 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/10/29 03:54:13 INFO storage.MemoryStore: MemoryStore cleared
18/10/29 03:54:13 INFO storage.BlockManager: BlockManager stopped
18/10/29 03:54:13 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
18/10/29 03:54:13 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitC
18/10/29 03:54:13 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
18/10/29 03:54:13 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proce
18/10/29 03:54:13 INFO spark.SparkContext: Successfully stopped SparkContext
18/10/29 03:54:13 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
notroot@ubuntu:~$ 18/10/29 03:54:14 INFO util.ShutdownHookManager: Shutdown hook called
18/10/29 03:54:14 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-f63a963a-2d9a-41e9-8d85-
18/10/29 03:54:14 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-f63a963a-2d9a-41e9-8d85-

notroot@ubuntu:~$
notroot@ubuntu:~$
notroot@ubuntu:~$ spark-submit --class com.jpmc.WordCount lab/programs/JavaWC.jar file:///home/notroot/
18/10/29 03:54:47 INFO spark.SparkContext: Running Spark version 1.6.2
18/10/29 03:54:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform...
18/10/29 03:54:48 WARN util.Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; usi
18/10/29 03:54:48 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/10/29 03:54:48 INFO spark.SecurityManager: Changing view acls to: notroot
18/10/29 03:54:48 INFO spark.SecurityManager: Changing modify acls to: notroot
18/10/29 03:54:48 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disableermissions: Set(notroot)
18/10/29 03:54:48 INFO util.Utils: Successfully started service 'sparkDriver' on port 57105.
18/10/29 03:54:49 INFO slf4j.Slf4jLogger: Slf4jLogger started
18/10/29 03:54:49 INFO Remoting: Starting remoting
18/10/29 03:54:49 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActor
18/10/29 03:54:49 INFO util.Utils: Successfully started service 'sparkDriverActorSystem' on port 46813.
18/10/29 03:54:49 INFO spark.SparkEnv: Registering MapOutputTracker
18/10/29 03:54:49 INFO spark.SparkEnv: Registering BlockManagerMaster
18/10/29 03:54:49 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-cef13c00-cd6f
18/10/29 03:54:49 INFO storage.MemoryStore: MemoryStore started with capacity 511.5 MB
18/10/29 03:54:49 INFO spark.SparkEnv: Registering OutputCommitCoordinator
18/10/29 03:54:49 INFO server.Server: jetty-8.y.z-SNAPSHOT
18/10/29 03:54:49 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040
18/10/29 03:54:49 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
18/10/29 03:54:49 INFO ui.SparkUI: Started SparkUI at http://192.168.150.143:4040
18/10/29 03:54:49 INFO spark.HttpFileServer: HTTP File server directory is /tmp/spark-256d5ad4-2a18-418
18/10/29 03:54:49 INFO spark.HttpServer: Starting HTTP Server
18/10/29 03:54:49 INFO server.Server: jetty-8.y.z-SNAPSHOT
18/10/29 03:54:49 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:41795
18/10/29 03:54:49 INFO util.Utils: Successfully started service 'HTTP file server' on port 41795.
18/10/29 03:54:49 INFO spark.SparkContext: Added JAR file:/home/notroot/lab/programs/JavaWC.jar at http9696
18/10/29 03:54:49 INFO executor.Executor: Starting executor ID driver on host localhost
18/10/29 03:54:49 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBl
18/10/29 03:54:49 INFO netty.NettyBlockTransferService: Server created on 40395
18/10/29 03:54:49 INFO storage.BlockManagerMaster: Trying to register BlockManager
18/10/29 03:54:49 INFO storage.BlockManagerMasterEndpoint: Registering block manager localhost:40395 wi
18/10/29 03:54:49 INFO storage.BlockManagerMaster: Registered BlockManager
18/10/29 03:54:50 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated siz
18/10/29 03:54:50 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimat
18/10/29 03:54:50 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:40395
18/10/29 03:54:50 INFO spark.SparkContext: Created broadcast 0 from textFile at WordCount.java:48
18/10/29 03:54:51 INFO mapred.FileInputFormat: Total input paths to process : 1
18/10/29 03:54:51 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.t
18/10/29 03:54:51 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.
18/10/29 03:54:51 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapred
18/10/29 03:54:51 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use map
18/10/29 03:54:51 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.j
18/10/29 03:54:51 INFO spark.SparkContext: Starting job: saveAsTextFile at WordCount.java:53
18/10/29 03:54:51 INFO scheduler.DAGScheduler: Registering RDD 3 (mapToPair at WordCount.java:50)
18/10/29 03:54:51 INFO scheduler.DAGScheduler: Got job 0 (saveAsTextFile at WordCount.java:53) with 1 o
18/10/29 03:54:51 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (saveAsTextFile at WordCount.
18/10/29 03:54:51 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
18/10/29 03:54:51 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 0)
18/10/29 03:54:51 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map
18/10/29 03:54:51 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated siz
18/10/29 03:54:51 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimat
18/10/29 03:54:51 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:40395
18/10/29 03:54:51 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:100
18/10/29 03:54:51 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPa
18/10/29 03:54:51 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
18/10/29 03:54:51 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, part
18/10/29 03:54:51 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
18/10/29 03:54:51 INFO executor.Executor: Fetching http://192.168.150.143:41795/jars/JavaWC.jar with ti
18/10/29 03:54:51 INFO util.Utils: Fetching http://192.168.150.143:41795/jars/JavaWC.jar to /tmp/spark--a154-ec83eb9eaf28/fetchFileTemp667554600253520242.tmp
18/10/29 03:54:51 INFO executor.Executor: Adding file:/tmp/spark-256d5ad4-2a18-4189-9a86-f4d93cfba6d5/u loader
18/10/29 03:54:51 INFO rdd.HadoopRDD: Input split: file:/home/notroot/lab/data/sample:0+37
18/10/29 03:54:51 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2253 bytes result sen
18/10/29 03:54:51 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 196 ms on lo
18/10/29 03:54:51 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (mapToPair at WordCount.java:50) finis
18/10/29 03:54:51 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/10/29 03:54:51 INFO scheduler.DAGScheduler: running: Set()
18/10/29 03:54:51 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 1)
18/10/29 03:54:51 INFO scheduler.DAGScheduler: failed: Set()
18/10/29 03:54:51 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at saveAsT
18/10/29 03:54:51 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed
18/10/29 03:54:51 INFO storage.MemoryStore: Block broadcast_2 stored as values in memory (estimated siz
18/10/29 03:54:51 INFO storage.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimat
18/10/29 03:54:51 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:40395
18/10/29 03:54:51 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:100
18/10/29 03:54:51 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartit
18/10/29 03:54:51 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
18/10/29 03:54:51 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, part
18/10/29 03:54:51 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)
18/10/29 03:54:52 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
18/10/29 03:54:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
18/10/29 03:54:52 INFO output.FileOutputCommitter: Saved output of task 'attempt_201810290354_0001_m_00201810290354_0001_m_000000
18/10/29 03:54:52 INFO mapred.SparkHadoopMapRedUtil: attempt_201810290354_0001_m_000000_1: Committed
18/10/29 03:54:52 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 2080 bytes result sen
18/10/29 03:54:52 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 857 ms on lo
18/10/29 03:54:52 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed
18/10/29 03:54:52 INFO scheduler.DAGScheduler: ResultStage 1 (saveAsTextFile at WordCount.java:53) fini
18/10/29 03:54:52 INFO scheduler.DAGScheduler: Job 0 finished: saveAsTextFile at WordCount.java:53, too
18/10/29 03:54:52 INFO spark.SparkContext: Invoking stop() from shutdown hook
18/10/29 03:54:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null
18/10/29 03:54:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill
18/10/29 03:54:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}
18/10/29 03:54:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
18/10/29 03:54:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
18/10/29 03:54:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadD
18/10/29 03:54:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadD
18/10/29 03:54:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,nu
18/10/29 03:54:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
18/10/29 03:54:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,
18/10/29 03:54:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
18/10/29 03:54:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,
18/10/29 03:54:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
18/10/29 03:54:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null
18/10/29 03:54:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}
18/10/29 03:54:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,
18/10/29 03:54:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
18/10/29 03:54:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json
18/10/29 03:54:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null
18/10/29 03:54:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
18/10/29 03:54:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}
18/10/29 03:54:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,nul
18/10/29 03:54:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
18/10/29 03:54:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
18/10/29 03:54:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}
18/10/29 03:54:52 INFO ui.SparkUI: Stopped Spark web UI at http://192.168.150.143:4040
18/10/29 03:54:52 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/10/29 03:54:52 INFO storage.MemoryStore: MemoryStore cleared
18/10/29 03:54:52 INFO storage.BlockManager: BlockManager stopped
18/10/29 03:54:52 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
18/10/29 03:54:52 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitC
18/10/29 03:54:52 INFO spark.SparkContext: Successfully stopped SparkContext
18/10/29 03:54:52 INFO util.ShutdownHookManager: Shutdown hook called
18/10/29 03:54:52 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-256d5ad4-2a18-4189-9a86-
18/10/29 03:54:52 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-256d5ad4-2a18-4189-9a86-
18/10/29 03:54:52 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
18/10/29 03:54:52 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proce
18/10/29 03:54:53 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
notroot@ubuntu:~$ hadoop -fs
Error: No command named `-fs' was found. Perhaps you meant `hadoop fs'
notroot@ubuntu:~$ hadoop -ls
Error: No command named `-ls' was found. Perhaps you meant `hadoop ls'
notroot@ubuntu:~$ jps
2511 JobHistoryServer
1567 NameNode
2066 ResourceManager
1884 SecondaryNameNode
2179 NodeManager
1706 DataNode
4717 Jps
notroot@ubuntu:~$ hdfs dfs -mkdir /sparkevents
notroot@ubuntu:~$ hdfs ls
Error: Could not find or load main class ls
notroot@ubuntu:~$ hdfs dfs ls
ls: Unknown command
Did you mean -ls?  This command begins with a dash.
notroot@ubuntu:~$ hdfs dfs -ls
ls: `.': No such file or directory
notroot@ubuntu:~$ hdfs dfs -ls
ls: `.': No such file or directory
notroot@ubuntu:~$ hdfs dfs dir
dir: Unknown command
notroot@ubuntu:~$ ./start-history-server.sh
-bash: ./start-history-server.sh: No such file or directory
notroot@ubuntu:~$ cd spark/sbin
-bash: cd: spark/sbin: No such file or directory
notroot@ubuntu:~$ cd lab/software/spark-1.6.2-bin-hadoop2.6/sbin/
notroot@ubuntu:~/lab/software/spark-1.6.2-bin-hadoop2.6/sbin$ ./st
start-all.sh                    start-mesos-shuffle-service.sh  start-thriftserver.sh           stop-me
start-history-server.sh         start-shuffle-service.sh        stop-all.sh                     stop-me
start-master.sh                 start-slave.sh                  stop-history-server.sh          stop-sh
start-mesos-dispatcher.sh       start-slaves.sh                 stop-master.sh                  stop-sl
notroot@ubuntu:~/lab/software/spark-1.6.2-bin-hadoop2.6/sbin$ ./start-history-server.sh
starting org.apache.spark.deploy.history.HistoryServer, logging to /home/notroot/lab/software/spark-1.6ry.HistoryServer-1-ubuntu.out
notroot@ubuntu:~/lab/software/spark-1.6.2-bin-hadoop2.6/sbin$ jps
2511 JobHistoryServer
4978 HistoryServer
1567 NameNode
2066 ResourceManager
1884 SecondaryNameNode
2179 NodeManager
5030 Jps
1706 DataNode
notroot@ubuntu:~/lab/software/spark-1.6.2-bin-hadoop2.6/sbin$ spark-submit --class com.jpmc.WordCount lWCResults
Warning: Local jar /home/notroot/lab/software/spark-1.6.2-bin-hadoop2.6/sbin/lab/programs/JavaWC.jar do
java.lang.ClassNotFoundException: com.jpmc.WordCount
        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:274)
        at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmi
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
notroot@ubuntu:~/lab/software/spark-1.6.2-bin-hadoop2.6/sbin$ cd /home/notroot/
notroot@ubuntu:~$ spark-submit --class com.jpmc.WordCount lab/programs/JavaWC.jar file:///home/notroot/
18/10/29 04:19:55 INFO spark.SparkContext: Running Spark version 1.6.2
18/10/29 04:19:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform...
18/10/29 04:19:55 WARN util.Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; usi
18/10/29 04:19:55 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/10/29 04:19:55 INFO spark.SecurityManager: Changing view acls to: notroot
18/10/29 04:19:55 INFO spark.SecurityManager: Changing modify acls to: notroot
18/10/29 04:19:55 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disableermissions: Set(notroot)
18/10/29 04:19:56 INFO util.Utils: Successfully started service 'sparkDriver' on port 42679.
18/10/29 04:19:56 INFO slf4j.Slf4jLogger: Slf4jLogger started
18/10/29 04:19:56 INFO Remoting: Starting remoting
18/10/29 04:19:56 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActor
18/10/29 04:19:57 INFO util.Utils: Successfully started service 'sparkDriverActorSystem' on port 58817.
18/10/29 04:19:57 INFO spark.SparkEnv: Registering MapOutputTracker
18/10/29 04:19:57 INFO spark.SparkEnv: Registering BlockManagerMaster
18/10/29 04:19:57 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-1042dd82-65e5
18/10/29 04:19:57 INFO storage.MemoryStore: MemoryStore started with capacity 511.5 MB
18/10/29 04:19:57 INFO spark.SparkEnv: Registering OutputCommitCoordinator
18/10/29 04:19:57 INFO server.Server: jetty-8.y.z-SNAPSHOT
18/10/29 04:19:57 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040
18/10/29 04:19:57 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
18/10/29 04:19:57 INFO ui.SparkUI: Started SparkUI at http://192.168.150.143:4040
18/10/29 04:19:57 INFO spark.HttpFileServer: HTTP File server directory is /tmp/spark-83e2a134-46dd-40a
18/10/29 04:19:57 INFO spark.HttpServer: Starting HTTP Server
18/10/29 04:19:57 INFO server.Server: jetty-8.y.z-SNAPSHOT
18/10/29 04:19:57 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:46364
18/10/29 04:19:57 INFO util.Utils: Successfully started service 'HTTP file server' on port 46364.
18/10/29 04:19:57 INFO spark.SparkContext: Added JAR file:/home/notroot/lab/programs/JavaWC.jar at http7484
18/10/29 04:19:57 INFO executor.Executor: Starting executor ID driver on host localhost
18/10/29 04:19:57 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBl
18/10/29 04:19:57 INFO netty.NettyBlockTransferService: Server created on 57303
18/10/29 04:19:57 INFO storage.BlockManagerMaster: Trying to register BlockManager
18/10/29 04:19:57 INFO storage.BlockManagerMasterEndpoint: Registering block manager localhost:57303 wi
18/10/29 04:19:57 INFO storage.BlockManagerMaster: Registered BlockManager
18/10/29 04:19:58 INFO scheduler.EventLoggingListener: Logging events to hdfs://localhost:9000/sparkeve
18/10/29 04:19:59 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated siz
18/10/29 04:19:59 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimat
18/10/29 04:19:59 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:57303
18/10/29 04:19:59 INFO spark.SparkContext: Created broadcast 0 from textFile at WordCount.java:48
18/10/29 04:19:59 INFO mapred.FileInputFormat: Total input paths to process : 1
Exception in thread "main" org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs:/
        at org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFun
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
        at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1154)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFuncti
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.sca
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.sca
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
        at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFuncti
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.sca
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.sca
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
        at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:951)
        at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1457)
        at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1436)
        at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1436)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
        at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1436)
        at org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:507)
        at org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)
        at com.jpmc.WordCount.main(WordCount.java:53)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmi
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
18/10/29 04:19:59 INFO spark.SparkContext: Invoking stop() from shutdown hook
18/10/29 04:19:59 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null
18/10/29 04:19:59 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill
18/10/29 04:19:59 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}
18/10/29 04:19:59 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
18/10/29 04:19:59 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
18/10/29 04:19:59 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadD
18/10/29 04:19:59 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadD
18/10/29 04:19:59 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,nu
18/10/29 04:19:59 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
18/10/29 04:19:59 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,
18/10/29 04:19:59 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
18/10/29 04:19:59 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,
18/10/29 04:19:59 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
18/10/29 04:19:59 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null
18/10/29 04:19:59 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}
18/10/29 04:19:59 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,
18/10/29 04:19:59 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
18/10/29 04:19:59 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json
18/10/29 04:19:59 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null
18/10/29 04:19:59 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
18/10/29 04:19:59 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}
18/10/29 04:19:59 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,nul
18/10/29 04:19:59 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
18/10/29 04:19:59 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
18/10/29 04:19:59 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}
18/10/29 04:19:59 INFO ui.SparkUI: Stopped Spark web UI at http://192.168.150.143:4040
18/10/29 04:19:59 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/10/29 04:19:59 INFO storage.MemoryStore: MemoryStore cleared
18/10/29 04:19:59 INFO storage.BlockManager: BlockManager stopped
18/10/29 04:19:59 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
18/10/29 04:20:00 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
18/10/29 04:20:00 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitC
18/10/29 04:20:00 INFO spark.SparkContext: Successfully stopped SparkContext
18/10/29 04:20:00 INFO util.ShutdownHookManager: Shutdown hook called
18/10/29 04:20:00 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-83e2a134-46dd-40ac-9d1e-
18/10/29 04:20:00 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-83e2a134-46dd-40ac-9d1e-
18/10/29 04:20:00 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proce
notroot@ubuntu:~$ spark-submit --class com.jpmc.WordCount lab/programs/JavaWC.jar file:///home/notroot/
18/10/29 04:20:55 INFO spark.SparkContext: Running Spark version 1.6.2
18/10/29 04:20:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform...
18/10/29 04:20:56 WARN util.Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; usi
18/10/29 04:20:56 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/10/29 04:20:56 INFO spark.SecurityManager: Changing view acls to: notroot
18/10/29 04:20:56 INFO spark.SecurityManager: Changing modify acls to: notroot
18/10/29 04:20:56 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disableermissions: Set(notroot)
18/10/29 04:20:57 INFO util.Utils: Successfully started service 'sparkDriver' on port 55595.
18/10/29 04:20:57 INFO slf4j.Slf4jLogger: Slf4jLogger started
18/10/29 04:20:57 INFO Remoting: Starting remoting
18/10/29 04:20:57 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActor
18/10/29 04:20:57 INFO util.Utils: Successfully started service 'sparkDriverActorSystem' on port 37108.
18/10/29 04:20:57 INFO spark.SparkEnv: Registering MapOutputTracker
18/10/29 04:20:57 INFO spark.SparkEnv: Registering BlockManagerMaster
18/10/29 04:20:57 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-8b051aaa-aebf
18/10/29 04:20:57 INFO storage.MemoryStore: MemoryStore started with capacity 511.5 MB
18/10/29 04:20:57 INFO spark.SparkEnv: Registering OutputCommitCoordinator
18/10/29 04:20:57 INFO server.Server: jetty-8.y.z-SNAPSHOT
18/10/29 04:20:57 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040
18/10/29 04:20:57 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
18/10/29 04:20:57 INFO ui.SparkUI: Started SparkUI at http://192.168.150.143:4040
18/10/29 04:20:57 INFO spark.HttpFileServer: HTTP File server directory is /tmp/spark-f3dc746f-f254-429
18/10/29 04:20:57 INFO spark.HttpServer: Starting HTTP Server
18/10/29 04:20:57 INFO server.Server: jetty-8.y.z-SNAPSHOT
18/10/29 04:20:57 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:53399
18/10/29 04:20:57 INFO util.Utils: Successfully started service 'HTTP file server' on port 53399.
18/10/29 04:20:58 INFO spark.SparkContext: Added JAR file:/home/notroot/lab/programs/JavaWC.jar at http8003
18/10/29 04:20:58 INFO executor.Executor: Starting executor ID driver on host localhost
18/10/29 04:20:58 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBl
18/10/29 04:20:58 INFO netty.NettyBlockTransferService: Server created on 37884
18/10/29 04:20:58 INFO storage.BlockManagerMaster: Trying to register BlockManager
18/10/29 04:20:58 INFO storage.BlockManagerMasterEndpoint: Registering block manager localhost:37884 wi
18/10/29 04:20:58 INFO storage.BlockManagerMaster: Registered BlockManager
18/10/29 04:20:59 INFO scheduler.EventLoggingListener: Logging events to hdfs://localhost:9000/sparkeve
18/10/29 04:20:59 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated siz
18/10/29 04:20:59 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimat
18/10/29 04:20:59 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:37884
18/10/29 04:20:59 INFO spark.SparkContext: Created broadcast 0 from textFile at WordCount.java:48
18/10/29 04:20:59 INFO mapred.FileInputFormat: Total input paths to process : 1
18/10/29 04:20:59 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.t
18/10/29 04:20:59 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.
18/10/29 04:20:59 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapred
18/10/29 04:20:59 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use map
18/10/29 04:20:59 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.j
18/10/29 04:20:59 INFO spark.SparkContext: Starting job: saveAsTextFile at WordCount.java:53
18/10/29 04:20:59 INFO scheduler.DAGScheduler: Registering RDD 3 (mapToPair at WordCount.java:50)
18/10/29 04:20:59 INFO scheduler.DAGScheduler: Got job 0 (saveAsTextFile at WordCount.java:53) with 1 o
18/10/29 04:20:59 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (saveAsTextFile at WordCount.
18/10/29 04:20:59 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
18/10/29 04:20:59 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 0)
18/10/29 04:20:59 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map
18/10/29 04:21:00 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated siz
18/10/29 04:21:00 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimat
18/10/29 04:21:00 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:37884
18/10/29 04:21:00 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:100
18/10/29 04:21:00 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPa
18/10/29 04:21:00 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
18/10/29 04:21:00 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, part
18/10/29 04:21:00 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
18/10/29 04:21:00 INFO executor.Executor: Fetching http://192.168.150.143:53399/jars/JavaWC.jar with ti
18/10/29 04:21:00 INFO util.Utils: Fetching http://192.168.150.143:53399/jars/JavaWC.jar to /tmp/spark--9129-6444d20c406a/fetchFileTemp2261882556836784435.tmp
18/10/29 04:21:00 INFO executor.Executor: Adding file:/tmp/spark-f3dc746f-f254-429a-b2f7-a07668af5a36/u loader
18/10/29 04:21:00 INFO rdd.HadoopRDD: Input split: file:/home/notroot/lab/data/sample:0+37
18/10/29 04:21:00 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2253 bytes result sen
18/10/29 04:21:00 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 220 ms on lo
18/10/29 04:21:00 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (mapToPair at WordCount.java:50) finis
18/10/29 04:21:00 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/10/29 04:21:00 INFO scheduler.DAGScheduler: running: Set()
18/10/29 04:21:00 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 1)
18/10/29 04:21:00 INFO scheduler.DAGScheduler: failed: Set()
18/10/29 04:21:00 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at saveAsT
18/10/29 04:21:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed
18/10/29 04:21:00 INFO storage.MemoryStore: Block broadcast_2 stored as values in memory (estimated siz
18/10/29 04:21:00 INFO storage.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimat
18/10/29 04:21:00 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:37884
18/10/29 04:21:00 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:100
18/10/29 04:21:00 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartit
18/10/29 04:21:00 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
18/10/29 04:21:00 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, part
18/10/29 04:21:00 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)
18/10/29 04:21:00 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
18/10/29 04:21:00 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
18/10/29 04:21:00 INFO output.FileOutputCommitter: Saved output of task 'attempt_201810290420_0001_m_00_201810290420_0001_m_000000
18/10/29 04:21:00 INFO mapred.SparkHadoopMapRedUtil: attempt_201810290420_0001_m_000000_1: Committed
18/10/29 04:21:00 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 2080 bytes result sen
18/10/29 04:21:00 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 142 ms on lo
18/10/29 04:21:00 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed
18/10/29 04:21:00 INFO scheduler.DAGScheduler: ResultStage 1 (saveAsTextFile at WordCount.java:53) fini
18/10/29 04:21:00 INFO scheduler.DAGScheduler: Job 0 finished: saveAsTextFile at WordCount.java:53, too
18/10/29 04:21:00 INFO spark.SparkContext: Invoking stop() from shutdown hook
18/10/29 04:21:00 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null
18/10/29 04:21:00 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill
18/10/29 04:21:00 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}
18/10/29 04:21:00 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
18/10/29 04:21:00 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
18/10/29 04:21:00 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadD
18/10/29 04:21:00 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadD
18/10/29 04:21:00 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,nu
18/10/29 04:21:00 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
18/10/29 04:21:00 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,
18/10/29 04:21:00 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
18/10/29 04:21:00 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,
18/10/29 04:21:00 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
18/10/29 04:21:00 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null
18/10/29 04:21:00 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}
18/10/29 04:21:00 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,
18/10/29 04:21:00 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
18/10/29 04:21:00 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json
18/10/29 04:21:00 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null
18/10/29 04:21:00 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
18/10/29 04:21:00 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}
18/10/29 04:21:00 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,nul
18/10/29 04:21:00 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
18/10/29 04:21:00 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
18/10/29 04:21:00 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}
18/10/29 04:21:00 INFO ui.SparkUI: Stopped Spark web UI at http://192.168.150.143:4040
18/10/29 04:21:00 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/10/29 04:21:00 INFO storage.MemoryStore: MemoryStore cleared
18/10/29 04:21:00 INFO storage.BlockManager: BlockManager stopped
18/10/29 04:21:00 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
18/10/29 04:21:00 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitC
18/10/29 04:21:00 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
18/10/29 04:21:00 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proce
18/10/29 04:21:00 INFO spark.SparkContext: Successfully stopped SparkContext
18/10/29 04:21:00 INFO util.ShutdownHookManager: Shutdown hook called
18/10/29 04:21:00 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-f3dc746f-f254-429a-b2f7-
18/10/29 04:21:00 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-f3dc746f-f254-429a-b2f7-
notroot@ubuntu:~$ spark-submit --class com.jpmc.WordCount lab/programs/JavaWC.jar file:///home/notroot/
18/10/29 04:22:12 INFO spark.SparkContext: Running Spark version 1.6.2
18/10/29 04:22:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform...
18/10/29 04:22:13 WARN util.Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; usi
18/10/29 04:22:13 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/10/29 04:22:13 INFO spark.SecurityManager: Changing view acls to: notroot
18/10/29 04:22:13 INFO spark.SecurityManager: Changing modify acls to: notroot
18/10/29 04:22:13 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disableermissions: Set(notroot)
18/10/29 04:22:13 INFO util.Utils: Successfully started service 'sparkDriver' on port 56340.
18/10/29 04:22:13 INFO slf4j.Slf4jLogger: Slf4jLogger started
18/10/29 04:22:13 INFO Remoting: Starting remoting
18/10/29 04:22:14 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActor
18/10/29 04:22:14 INFO util.Utils: Successfully started service 'sparkDriverActorSystem' on port 57669.
18/10/29 04:22:14 INFO spark.SparkEnv: Registering MapOutputTracker
18/10/29 04:22:14 INFO spark.SparkEnv: Registering BlockManagerMaster
18/10/29 04:22:14 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-c0ae6fc2-4223
18/10/29 04:22:14 INFO storage.MemoryStore: MemoryStore started with capacity 511.5 MB
18/10/29 04:22:14 INFO spark.SparkEnv: Registering OutputCommitCoordinator
18/10/29 04:22:18 INFO server.Server: jetty-8.y.z-SNAPSHOT
18/10/29 04:22:18 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040
18/10/29 04:22:18 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
18/10/29 04:22:18 INFO ui.SparkUI: Started SparkUI at http://192.168.150.143:4040
18/10/29 04:22:18 INFO spark.HttpFileServer: HTTP File server directory is /tmp/spark-38ab4383-0112-450
18/10/29 04:22:18 INFO spark.HttpServer: Starting HTTP Server
18/10/29 04:22:18 INFO server.Server: jetty-8.y.z-SNAPSHOT
18/10/29 04:22:18 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:39999
18/10/29 04:22:18 INFO util.Utils: Successfully started service 'HTTP file server' on port 39999.
18/10/29 04:22:18 INFO spark.SparkContext: Added JAR file:/home/notroot/lab/programs/JavaWC.jar at http8598
18/10/29 04:22:18 INFO executor.Executor: Starting executor ID driver on host localhost
18/10/29 04:22:18 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBl
18/10/29 04:22:18 INFO netty.NettyBlockTransferService: Server created on 45410
18/10/29 04:22:18 INFO storage.BlockManagerMaster: Trying to register BlockManager
18/10/29 04:22:18 INFO storage.BlockManagerMasterEndpoint: Registering block manager localhost:45410 wi
18/10/29 04:22:18 INFO storage.BlockManagerMaster: Registered BlockManager
18/10/29 04:22:20 INFO scheduler.EventLoggingListener: Logging events to hdfs://localhost:9000/sparkeve
18/10/29 04:22:20 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated siz
18/10/29 04:22:20 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimat
18/10/29 04:22:20 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:45410
18/10/29 04:22:20 INFO spark.SparkContext: Created broadcast 0 from textFile at WordCount.java:48
18/10/29 04:22:20 INFO mapred.FileInputFormat: Total input paths to process : 1
Exception in thread "main" org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs:/
        at org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFun
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
        at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1154)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFuncti
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.sca
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.sca
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
        at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFuncti
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.sca
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.sca
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
        at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:951)
        at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1457)
        at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1436)
        at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1436)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
        at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1436)
        at org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:507)
        at org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)
        at com.jpmc.WordCount.main(WordCount.java:53)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmi
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
18/10/29 04:22:20 INFO spark.SparkContext: Invoking stop() from shutdown hook
18/10/29 04:22:20 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null
18/10/29 04:22:20 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill
18/10/29 04:22:20 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}
18/10/29 04:22:20 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
18/10/29 04:22:20 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
18/10/29 04:22:20 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadD
18/10/29 04:22:20 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadD
18/10/29 04:22:20 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,nu
18/10/29 04:22:20 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
18/10/29 04:22:20 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,
18/10/29 04:22:20 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
18/10/29 04:22:20 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,
18/10/29 04:22:20 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
18/10/29 04:22:20 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null
18/10/29 04:22:20 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}
18/10/29 04:22:20 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,
18/10/29 04:22:20 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
18/10/29 04:22:20 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json
18/10/29 04:22:20 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null
18/10/29 04:22:20 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
18/10/29 04:22:20 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}
18/10/29 04:22:20 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,nul
18/10/29 04:22:20 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
18/10/29 04:22:20 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
18/10/29 04:22:20 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}
18/10/29 04:22:20 INFO ui.SparkUI: Stopped Spark web UI at http://192.168.150.143:4040
18/10/29 04:22:20 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/10/29 04:22:20 INFO storage.MemoryStore: MemoryStore cleared
18/10/29 04:22:20 INFO storage.BlockManager: BlockManager stopped
18/10/29 04:22:20 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
18/10/29 04:22:20 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitC
18/10/29 04:22:20 INFO spark.SparkContext: Successfully stopped SparkContext
18/10/29 04:22:20 INFO util.ShutdownHookManager: Shutdown hook called
18/10/29 04:22:20 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-38ab4383-0112-4508-ab82-
18/10/29 04:22:20 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-38ab4383-0112-4508-ab82-
18/10/29 04:22:20 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
18/10/29 04:22:20 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proce
notroot@ubuntu:~$ spark-submit --class com.jpmc.WordCount lab/programs/JavaWC.jar file:///home/notroot/
18/10/29 04:27:03 INFO spark.SparkContext: Running Spark version 1.6.2
18/10/29 04:27:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform...
18/10/29 04:27:03 WARN util.Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; usi
18/10/29 04:27:03 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/10/29 04:27:03 INFO spark.SecurityManager: Changing view acls to: notroot
18/10/29 04:27:03 INFO spark.SecurityManager: Changing modify acls to: notroot
18/10/29 04:27:03 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disableermissions: Set(notroot)
18/10/29 04:27:04 INFO util.Utils: Successfully started service 'sparkDriver' on port 57875.
18/10/29 04:27:04 INFO slf4j.Slf4jLogger: Slf4jLogger started
18/10/29 04:27:04 INFO Remoting: Starting remoting
18/10/29 04:27:04 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActor
18/10/29 04:27:04 INFO util.Utils: Successfully started service 'sparkDriverActorSystem' on port 34144.
18/10/29 04:27:04 INFO spark.SparkEnv: Registering MapOutputTracker
18/10/29 04:27:04 INFO spark.SparkEnv: Registering BlockManagerMaster
18/10/29 04:27:04 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-97a48d36-8dd2
18/10/29 04:27:04 INFO storage.MemoryStore: MemoryStore started with capacity 511.5 MB
18/10/29 04:27:04 INFO spark.SparkEnv: Registering OutputCommitCoordinator
18/10/29 04:27:05 INFO server.Server: jetty-8.y.z-SNAPSHOT
18/10/29 04:27:05 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040
18/10/29 04:27:05 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
18/10/29 04:27:05 INFO ui.SparkUI: Started SparkUI at http://192.168.150.143:4040
18/10/29 04:27:05 INFO spark.HttpFileServer: HTTP File server directory is /tmp/spark-0394da75-2e46-497
18/10/29 04:27:05 INFO spark.HttpServer: Starting HTTP Server
18/10/29 04:27:05 INFO server.Server: jetty-8.y.z-SNAPSHOT
18/10/29 04:27:05 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:38667
18/10/29 04:27:05 INFO util.Utils: Successfully started service 'HTTP file server' on port 38667.
18/10/29 04:27:05 INFO spark.SparkContext: Added JAR file:/home/notroot/lab/programs/JavaWC.jar at http5255
18/10/29 04:27:05 INFO executor.Executor: Starting executor ID driver on host localhost
18/10/29 04:27:05 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBl
18/10/29 04:27:05 INFO netty.NettyBlockTransferService: Server created on 40743
18/10/29 04:27:05 INFO storage.BlockManagerMaster: Trying to register BlockManager
18/10/29 04:27:05 INFO storage.BlockManagerMasterEndpoint: Registering block manager localhost:40743 wi
18/10/29 04:27:05 INFO storage.BlockManagerMaster: Registered BlockManager
18/10/29 04:27:06 INFO scheduler.EventLoggingListener: Logging events to hdfs://localhost:9000/sparkeve
18/10/29 04:27:07 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated siz
18/10/29 04:27:07 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimat
18/10/29 04:27:07 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:40743
18/10/29 04:27:07 INFO spark.SparkContext: Created broadcast 0 from textFile at WordCount.java:48
18/10/29 04:27:07 INFO mapred.FileInputFormat: Total input paths to process : 1
18/10/29 04:27:07 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.t
18/10/29 04:27:07 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.
18/10/29 04:27:07 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapred
18/10/29 04:27:07 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use map
18/10/29 04:27:07 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.j
18/10/29 04:27:07 INFO spark.SparkContext: Starting job: saveAsTextFile at WordCount.java:53
18/10/29 04:27:07 INFO scheduler.DAGScheduler: Registering RDD 3 (mapToPair at WordCount.java:50)
18/10/29 04:27:07 INFO scheduler.DAGScheduler: Got job 0 (saveAsTextFile at WordCount.java:53) with 1 o
18/10/29 04:27:07 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (saveAsTextFile at WordCount.
18/10/29 04:27:07 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
18/10/29 04:27:07 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 0)
18/10/29 04:27:07 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map
18/10/29 04:27:07 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated siz
18/10/29 04:27:07 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimat
18/10/29 04:27:07 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:40743
18/10/29 04:27:07 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:100
18/10/29 04:27:07 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPa
18/10/29 04:27:07 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
18/10/29 04:27:07 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, part
18/10/29 04:27:07 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
18/10/29 04:27:07 INFO executor.Executor: Fetching http://192.168.150.143:38667/jars/JavaWC.jar with ti
18/10/29 04:27:07 INFO util.Utils: Fetching http://192.168.150.143:38667/jars/JavaWC.jar to /tmp/spark--98e6-f6336e888896/fetchFileTemp8333571027178170343.tmp
18/10/29 04:27:07 INFO executor.Executor: Adding file:/tmp/spark-0394da75-2e46-4975-8db5-57d750665a76/u loader
18/10/29 04:27:07 INFO rdd.HadoopRDD: Input split: file:/home/notroot/lab/data/sample:0+37
18/10/29 04:27:07 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2253 bytes result sen
18/10/29 04:27:07 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (mapToPair at WordCount.java:50) finis
18/10/29 04:27:07 INFO scheduler.DAGScheduler: looking for newly runnable stages
18/10/29 04:27:07 INFO scheduler.DAGScheduler: running: Set()
18/10/29 04:27:07 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 280 ms on lo
18/10/29 04:27:07 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 1)
18/10/29 04:27:07 INFO scheduler.DAGScheduler: failed: Set()
18/10/29 04:27:07 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at saveAsT
18/10/29 04:27:07 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed
18/10/29 04:27:08 INFO storage.MemoryStore: Block broadcast_2 stored as values in memory (estimated siz
18/10/29 04:27:08 INFO storage.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimat
18/10/29 04:27:08 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:40743
18/10/29 04:27:08 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:100
18/10/29 04:27:08 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartit
18/10/29 04:27:08 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
18/10/29 04:27:08 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, part
18/10/29 04:27:08 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)
18/10/29 04:27:08 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
18/10/29 04:27:08 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
18/10/29 04:27:08 INFO output.FileOutputCommitter: Saved output of task 'attempt_201810290427_0001_m_00_201810290427_0001_m_000000
18/10/29 04:27:08 INFO mapred.SparkHadoopMapRedUtil: attempt_201810290427_0001_m_000000_1: Committed
18/10/29 04:27:08 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 2080 bytes result sen
18/10/29 04:27:08 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 158 ms on lo
18/10/29 04:27:08 INFO scheduler.DAGScheduler: ResultStage 1 (saveAsTextFile at WordCount.java:53) fini
18/10/29 04:27:08 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed
18/10/29 04:27:08 INFO scheduler.DAGScheduler: Job 0 finished: saveAsTextFile at WordCount.java:53, too
18/10/29 04:27:08 INFO spark.SparkContext: Invoking stop() from shutdown hook
18/10/29 04:27:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null
18/10/29 04:27:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill
18/10/29 04:27:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}
18/10/29 04:27:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
18/10/29 04:27:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
18/10/29 04:27:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadD
18/10/29 04:27:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadD
18/10/29 04:27:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,nu
18/10/29 04:27:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
18/10/29 04:27:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,
18/10/29 04:27:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
18/10/29 04:27:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,
18/10/29 04:27:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
18/10/29 04:27:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null
18/10/29 04:27:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}
18/10/29 04:27:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,
18/10/29 04:27:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
18/10/29 04:27:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json
18/10/29 04:27:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null
18/10/29 04:27:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
18/10/29 04:27:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}
18/10/29 04:27:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,nul
18/10/29 04:27:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
18/10/29 04:27:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
18/10/29 04:27:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}
18/10/29 04:27:08 INFO ui.SparkUI: Stopped Spark web UI at http://192.168.150.143:4040
18/10/29 04:27:08 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/10/29 04:27:08 INFO storage.MemoryStore: MemoryStore cleared
18/10/29 04:27:08 INFO storage.BlockManager: BlockManager stopped
18/10/29 04:27:08 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
18/10/29 04:27:08 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitC
18/10/29 04:27:08 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
18/10/29 04:27:08 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proce
18/10/29 04:27:08 INFO spark.SparkContext: Successfully stopped SparkContext
18/10/29 04:27:08 INFO util.ShutdownHookManager: Shutdown hook called
18/10/29 04:27:08 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-0394da75-2e46-4975-8db5-
18/10/29 04:27:08 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-0394da75-2e46-4975-8db5-
notroot@ubuntu:~$ hdfs dfs –mkdir input
–mkdir: Unknown command
notroot@ubuntu:~$ hdfs dfs –mkdir /input
–mkdir: Unknown command
notroot@ubuntu:~$ hdfs dfs -mkdir /input
notroot@ubuntu:~$ hdfs dfs –put /input
–put: Unknown command
notroot@ubuntu:~$ hdfs dfs –put input
–put: Unknown command
notroot@ubuntu:~$ hdfs dfs -put input
put: `.': No such file or directory
notroot@ubuntu:~$ pwd
/home/notroot
notroot@ubuntu:~$ cd lab/data/
notroot@ubuntu:~/lab/data$ hdfs dfs -copyFromLocal sample /input
notroot@ubuntu:~/lab/data$ cd ..
notroot@ubuntu:~/lab$ pwd
/home/notroot/lab
notroot@ubuntu:~/lab$ ls
data  hdfs  programs  software
notroot@ubuntu:~/lab$ cd programs/
notroot@ubuntu:~/lab/programs$ spark-submit --class com.jpmc.ScalaFirst --master local ScalaFirst.jar
Warning: Local jar /home/notroot/lab/programs/ScalaFirst.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.jpmc.ScalaFirst
        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:274)
        at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmi
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
notroot@ubuntu:~/lab/programs$ spark-submit --class com.jpmc.ScalaFirst --master local JavaWC1.jar
Creating Spark Configuration
Creating Spark Context
18/10/29 04:59:59 INFO spark.SparkContext: Running Spark version 1.6.2
18/10/29 04:59:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform...
18/10/29 04:59:59 WARN util.Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; usi
18/10/29 04:59:59 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/10/29 04:59:59 INFO spark.SecurityManager: Changing view acls to: notroot
18/10/29 04:59:59 INFO spark.SecurityManager: Changing modify acls to: notroot
18/10/29 04:59:59 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disableermissions: Set(notroot)
18/10/29 05:00:00 INFO util.Utils: Successfully started service 'sparkDriver' on port 59254.
18/10/29 05:00:00 INFO slf4j.Slf4jLogger: Slf4jLogger started
18/10/29 05:00:00 INFO Remoting: Starting remoting
18/10/29 05:00:01 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActor
18/10/29 05:00:01 INFO util.Utils: Successfully started service 'sparkDriverActorSystem' on port 50800.
18/10/29 05:00:01 INFO spark.SparkEnv: Registering MapOutputTracker
18/10/29 05:00:01 INFO spark.SparkEnv: Registering BlockManagerMaster
18/10/29 05:00:01 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-e0797039-82e9
18/10/29 05:00:01 INFO storage.MemoryStore: MemoryStore started with capacity 511.5 MB
18/10/29 05:00:01 INFO spark.SparkEnv: Registering OutputCommitCoordinator
18/10/29 05:00:01 INFO server.Server: jetty-8.y.z-SNAPSHOT
18/10/29 05:00:01 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040
18/10/29 05:00:01 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
18/10/29 05:00:01 INFO ui.SparkUI: Started SparkUI at http://192.168.150.143:4040
18/10/29 05:00:01 INFO spark.HttpFileServer: HTTP File server directory is /tmp/spark-b1601196-256d-4a9
18/10/29 05:00:01 INFO spark.HttpServer: Starting HTTP Server
18/10/29 05:00:01 INFO server.Server: jetty-8.y.z-SNAPSHOT
18/10/29 05:00:01 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:38912
18/10/29 05:00:01 INFO util.Utils: Successfully started service 'HTTP file server' on port 38912.
18/10/29 05:00:01 INFO spark.SparkContext: Added JAR file:/home/notroot/lab/programs/JavaWC1.jar at htt401562
18/10/29 05:00:01 INFO executor.Executor: Starting executor ID driver on host localhost
18/10/29 05:00:01 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBl
18/10/29 05:00:01 INFO netty.NettyBlockTransferService: Server created on 44594
18/10/29 05:00:01 INFO storage.BlockManagerMaster: Trying to register BlockManager
18/10/29 05:00:01 INFO storage.BlockManagerMasterEndpoint: Registering block manager localhost:44594 wi
18/10/29 05:00:01 INFO storage.BlockManagerMaster: Registered BlockManager
18/10/29 05:00:03 INFO scheduler.EventLoggingListener: Logging events to hdfs://localhost:9000/sparkeve
Loading the Dataset and will further process it
18/10/29 05:00:04 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated siz
18/10/29 05:00:04 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimat
18/10/29 05:00:04 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:44594
18/10/29 05:00:04 INFO spark.SparkContext: Created broadcast 0 from textFile at ScalaFirst.scala:21
18/10/29 05:00:04 INFO mapred.FileInputFormat: Total input paths to process : 1
18/10/29 05:00:04 INFO spark.SparkContext: Starting job: count at ScalaFirst.scala:23
18/10/29 05:00:04 INFO scheduler.DAGScheduler: Got job 0 (count at ScalaFirst.scala:23) with 2 output p
18/10/29 05:00:04 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (count at ScalaFirst.scala:23
18/10/29 05:00:04 INFO scheduler.DAGScheduler: Parents of final stage: List()
18/10/29 05:00:04 INFO scheduler.DAGScheduler: Missing parents: List()
18/10/29 05:00:04 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at filter
18/10/29 05:00:04 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated siz
18/10/29 05:00:04 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimat
18/10/29 05:00:04 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:44594
18/10/29 05:00:04 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:100
18/10/29 05:00:04 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartit
18/10/29 05:00:04 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
18/10/29 05:00:04 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, part
18/10/29 05:00:04 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
18/10/29 05:00:04 INFO executor.Executor: Fetching http://192.168.150.143:38912/jars/JavaWC1.jar with t
18/10/29 05:00:04 INFO util.Utils: Fetching http://192.168.150.143:38912/jars/JavaWC1.jar to /tmp/spark5-93c1-4e44f89489d2/fetchFileTemp3992739883109137961.tmp
18/10/29 05:00:04 INFO executor.Executor: Adding file:/tmp/spark-b1601196-256d-4a97-9458-a80988c6bd36/us loader
18/10/29 05:00:04 INFO rdd.HadoopRDD: Input split: hdfs://localhost:9000/input/sample:0+18
18/10/29 05:00:04 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.t
18/10/29 05:00:04 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.
18/10/29 05:00:04 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapred
18/10/29 05:00:04 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use map
18/10/29 05:00:04 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.j
18/10/29 05:00:04 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2082 bytes result sen
18/10/29 05:00:04 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, part
18/10/29 05:00:04 INFO executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
18/10/29 05:00:04 INFO rdd.HadoopRDD: Input split: hdfs://localhost:9000/input/sample:18+19
18/10/29 05:00:04 INFO executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 2082 bytes result sen
18/10/29 05:00:04 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 219 ms on lo
18/10/29 05:00:04 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 42 ms on loc
18/10/29 05:00:04 INFO scheduler.DAGScheduler: ResultStage 0 (count at ScalaFirst.scala:23) finished in
18/10/29 05:00:04 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed
18/10/29 05:00:04 INFO scheduler.DAGScheduler: Job 0 finished: count at ScalaFirst.scala:23, took 0.405
Number of Lines in the Dataset 3
18/10/29 05:00:04 INFO spark.SparkContext: Invoking stop() from shutdown hook
18/10/29 05:00:04 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null
18/10/29 05:00:04 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill
18/10/29 05:00:04 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}
18/10/29 05:00:04 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
18/10/29 05:00:04 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
18/10/29 05:00:04 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadD
18/10/29 05:00:04 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadD
18/10/29 05:00:04 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,nu
18/10/29 05:00:04 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
18/10/29 05:00:04 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,
18/10/29 05:00:04 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
18/10/29 05:00:04 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,
18/10/29 05:00:04 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
18/10/29 05:00:04 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null
18/10/29 05:00:04 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}
18/10/29 05:00:04 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,
18/10/29 05:00:04 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
18/10/29 05:00:04 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json
18/10/29 05:00:04 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null
18/10/29 05:00:04 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
18/10/29 05:00:04 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}
18/10/29 05:00:04 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,nul
18/10/29 05:00:04 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
18/10/29 05:00:04 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
18/10/29 05:00:04 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}
18/10/29 05:00:04 INFO ui.SparkUI: Stopped Spark web UI at http://192.168.150.143:4040
18/10/29 05:00:04 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/10/29 05:00:04 INFO storage.MemoryStore: MemoryStore cleared
18/10/29 05:00:04 INFO storage.BlockManager: BlockManager stopped
18/10/29 05:00:04 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
18/10/29 05:00:04 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitC
18/10/29 05:00:04 INFO spark.SparkContext: Successfully stopped SparkContext
18/10/29 05:00:04 INFO util.ShutdownHookManager: Shutdown hook called
18/10/29 05:00:04 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-b1601196-256d-4a97-9458-
18/10/29 05:00:04 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-b1601196-256d-4a97-9458-
18/10/29 05:00:04 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
18/10/29 05:00:04 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proce
18/10/29 05:00:04 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
notroot@ubuntu:~/lab/programs$ spark-submit --class com.jpmc.ScalaFirst --master local JavaWC1.jar
Creating Spark Configuration
Creating Spark Context
Loading the Dataset and will further process it
Number of Lines in the Dataset 3
notroot@ubuntu:~/lab/programs$
notroot@ubuntu:~/lab/programs$
notroot@ubuntu:~/lab/programs$ cd /home/notroot/
notroot@ubuntu:~$ pwd
/home/notroot
notroot@ubuntu:~$ cd downloads/
notroot@ubuntu:~/downloads$ bash Anaconda3-4.0.0-Linux-x86_64.sh

Welcome to Anaconda3 4.0.0 (by Continuum Analytics, Inc.)

In order to continue the installation process, please review the license
agreement.
Please, press ENTER to continue
>>>
================
Anaconda License
================

Copyright 2016, Continuum Analytics, Inc.

All rights reserved under the 3-clause BSD License:

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

* Redistributions of source code must retain the above copyright notice,
this list of conditions and the following disclaimer.

* Redistributions in binary form must reproduce the above copyright notice,
this list of conditions and the following disclaimer in the documentation
and/or other materials provided with the distribution.

* Neither the name of Continuum Analytics, Inc. nor the names of its
contributors may be used to endorse or promote products derived from this
software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
ARE DISCLAIMED. IN NO EVENT SHALL CONTINUUM ANALYTICS, INC. BE LIABLE FOR
ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
DAMAGE.


Notice of Third Party Software Licenses
=======================================

Anaconda contains open source software packages from third parties. These
are available on an "as is" basis and subject to their individual license
agreements. These licenses are available in Anaconda or at
http://docs.continuum.io/anaconda/pkg-docs . Any binary packages of these
third party tools you obtain via Anaconda are subject to their individual
licenses as well as the Anaconda license. Continuum reserves the right to
change which third party tools are provided in Anaconda.

In particular, Anaconda contains re-distributable, run-time, shared-library
files from the Intel (TM) Math Kernel Library ("MKL binaries").  You are
specifically authorized to use the MKL binaries with your installation of
Anaconda.  You are also authorized to redistribute the MKL binaries with
Anaconda or in the conda package that contains them.  If needed,
instructions for removing the MKL binaries after installation of Anaconda
are available at http://www.continuum.io.

Cryptography Notice
===================
This distribution includes cryptographic software. The country in which you
currently reside may have restrictions on the import, possession, use,
and/or re-export to another country, of encryption software. BEFORE using
any encryption software, please check your country's laws, regulations and
policies concerning the import, possession, or use, and re-export of
encryption software, to see if this is permitted. See the Wassenaar
Arrangement <http://www.wassenaar.org/> for more information.

Continuum Analytics has self-classified this software as Export Commodity
Control Number (ECCN) 5D002.C.1, which includes information security
software using or performing cryptographic functions with asymmetric
algorithms. The form and manner of this distribution makes it eligible for
export under the License Exception ENC Technology Software Unrestricted
(TSU) exception (see the BIS Export Administration Regulations, Section
740.13) for both object code and source code.

The following packages are included in this distribution that relate to
cryptography:

openssl
The OpenSSL Project is a collaborative effort to develop a robust,
commercial-grade, full-featured, and Open Source toolkit implementing the
Transport Layer Security (TLS) and Secure Sockets Layer (SSL) protocols as
well as a full-strength general purpose cryptography library.

pycrypto
A collection of both secure hash functions (such as SHA256 and RIPEMD160),
and various encryption algorithms (AES, DES, RSA, ElGamal, etc.).

pyopenssl
A thin Python wrapper around (a subset of) the OpenSSL library.

kerberos (krb5, non-Windows platforms)
A network authentication protocol designed to provide strong authentication
for client/server applications by using secret-key cryptography.

cryptography
A Python library which exposes cryptographic recipes and primitives.

Do you approve the license terms? [yes|no]
>>>
Please answer 'yes' or 'no':
>>>
Please answer 'yes' or 'no':
>>>
Please answer 'yes' or 'no':
>>>
Please answer 'yes' or 'no':
>>>
Please answer 'yes' or 'no':
>>>
Please answer 'yes' or 'no':
>>>
Please answer 'yes' or 'no':
>>>
Please answer 'yes' or 'no':
>>>

Please answer 'yes' or 'no':
>>> Please answer 'yes' or 'no':
>>>
Please answer 'yes' or 'no':
>>>
Please answer 'yes' or 'no':
>>>
Please answer 'yes' or 'no':
>>>
Please answer 'yes' or 'no':
>>>
Please answer 'yes' or 'no':
>>>
Please answer 'yes' or 'no':
>>>
Please answer 'yes' or 'no':
>>>
Please answer 'yes' or 'no':
>>>
Please answer 'yes' or 'no':
>>>
Please answer 'yes' or 'no':
>>> yes

Anaconda3 will now be installed into this location:
/home/notroot/anaconda3

  - Press ENTER to confirm the location
  - Press CTRL-C to abort the installation
  - Or specify a different location below

[/home/notroot/anaconda3] >>>
PREFIX=/home/notroot/anaconda3
installing: _cache-0.0-py35_x0 ...
installing: python-3.5.1-0 ...
installing: alabaster-0.7.7-py35_0 ...
installing: anaconda-client-1.4.0-py35_0 ...
installing: anaconda-navigator-1.1.0-py35_0 ...
installing: argcomplete-1.0.0-py35_1 ...
installing: astropy-1.1.2-np110py35_0 ...
installing: babel-2.2.0-py35_0 ...
installing: beautifulsoup4-4.4.1-py35_0 ...
installing: bitarray-0.8.1-py35_0 ...
installing: blaze-0.9.1-py35_0 ...
installing: bokeh-0.11.1-py35_0 ...
installing: boto-2.39.0-py35_0 ...
installing: bottleneck-1.0.0-np110py35_0 ...
installing: cffi-1.5.2-py35_0 ...
installing: chest-0.2.3-py35_0 ...
installing: cloudpickle-0.1.1-py35_0 ...
installing: clyent-1.2.1-py35_0 ...
installing: colorama-0.3.7-py35_0 ...
installing: conda-manager-0.3.1-py35_0 ...
installing: configobj-5.0.6-py35_0 ...
installing: cryptography-1.3-py35_0 ...
installing: curl-7.45.0-0 ...
installing: cycler-0.10.0-py35_0 ...
installing: cython-0.23.4-py35_0 ...
installing: cytoolz-0.7.5-py35_0 ...
installing: dask-0.8.1-py35_0 ...
installing: datashape-0.5.1-py35_0 ...
installing: decorator-4.0.9-py35_0 ...
installing: dill-0.2.4-py35_0 ...
installing: docutils-0.12-py35_0 ...
installing: dynd-python-0.7.2-py35_0 ...
installing: et_xmlfile-1.0.1-py35_0 ...
installing: fastcache-1.0.2-py35_0 ...
installing: flask-0.10.1-py35_1 ...
installing: flask-cors-2.1.2-py35_0 ...
installing: fontconfig-2.11.1-5 ...
installing: freetype-2.5.5-0 ...
installing: gevent-1.1.0-py35_0 ...
installing: greenlet-0.4.9-py35_0 ...
installing: h5py-2.5.0-np110py35_4 ...
installing: hdf5-1.8.15.1-2 ...
installing: heapdict-1.0.0-py35_0 ...
installing: idna-2.0-py35_0 ...
installing: ipykernel-4.3.1-py35_0 ...
installing: ipython-4.1.2-py35_1 ...
installing: ipython_genutils-0.1.0-py35_0 ...
installing: ipywidgets-4.1.1-py35_0 ...
installing: itsdangerous-0.24-py35_0 ...
installing: jbig-2.1-0 ...
installing: jdcal-1.2-py35_0 ...
installing: jedi-0.9.0-py35_0 ...
installing: jinja2-2.8-py35_0 ...
installing: jpeg-8d-0 ...
installing: jsonschema-2.4.0-py35_0 ...
installing: jupyter-1.0.0-py35_2 ...
installing: jupyter_client-4.2.2-py35_0 ...
installing: jupyter_console-4.1.1-py35_0 ...
installing: jupyter_core-4.1.0-py35_0 ...
installing: libdynd-0.7.2-0 ...
installing: libffi-3.0.13-0 ...
installing: libgfortran-3.0-0 ...
installing: libpng-1.6.17-0 ...
installing: libsodium-1.0.3-0 ...
installing: libtiff-4.0.6-1 ...
installing: libxml2-2.9.2-0 ...
installing: libxslt-1.1.28-0 ...
installing: llvmlite-0.9.0-py35_0 ...
installing: locket-0.2.0-py35_0 ...
installing: lxml-3.6.0-py35_0 ...
installing: markupsafe-0.23-py35_0 ...
installing: matplotlib-1.5.1-np110py35_0 ...
installing: mistune-0.7.2-py35_0 ...
installing: mkl-11.3.1-0 ...
installing: mkl-service-1.1.2-py35_0 ...
installing: mpmath-0.19-py35_0 ...
installing: multipledispatch-0.4.8-py35_0 ...
installing: nbconvert-4.1.0-py35_0 ...
installing: nbformat-4.0.1-py35_0 ...
installing: networkx-1.11-py35_0 ...
installing: nltk-3.2-py35_0 ...
installing: nose-1.3.7-py35_0 ...
installing: notebook-4.1.0-py35_1 ...
installing: numba-0.24.0-np110py35_0 ...
installing: numexpr-2.5-np110py35_0 ...
installing: numpy-1.10.4-py35_1 ...
installing: odo-0.4.2-py35_0 ...
installing: openpyxl-2.3.2-py35_0 ...
installing: openssl-1.0.2g-0 ...
installing: pandas-0.18.0-np110py35_0 ...
installing: partd-0.3.2-py35_1 ...
installing: patchelf-0.8-0 ...
installing: path.py-8.1.2-py35_1 ...
installing: patsy-0.4.0-np110py35_0 ...
installing: pep8-1.7.0-py35_0 ...
installing: pexpect-4.0.1-py35_0 ...
installing: pickleshare-0.5-py35_0 ...
installing: pillow-3.1.1-py35_0 ...
installing: pip-8.1.1-py35_1 ...
installing: ply-3.8-py35_0 ...
installing: psutil-4.1.0-py35_0 ...
installing: ptyprocess-0.5-py35_0 ...
installing: py-1.4.31-py35_0 ...
installing: pyasn1-0.1.9-py35_0 ...
installing: pycosat-0.6.1-py35_0 ...
installing: pycparser-2.14-py35_0 ...
installing: pycrypto-2.6.1-py35_0 ...
installing: pycurl-7.19.5.3-py35_0 ...
installing: pyflakes-1.1.0-py35_0 ...
installing: pygments-2.1.1-py35_0 ...
installing: pyopenssl-0.15.1-py35_2 ...
installing: pyparsing-2.0.3-py35_0 ...
installing: pyqt-4.11.4-py35_1 ...
installing: pytables-3.2.2-np110py35_1 ...
installing: pytest-2.8.5-py35_0 ...
installing: python-dateutil-2.5.1-py35_0 ...
installing: pytz-2016.2-py35_0 ...
installing: pyyaml-3.11-py35_1 ...
installing: pyzmq-15.2.0-py35_0 ...
installing: qt-4.8.7-1 ...
installing: qtawesome-0.3.2-py35_0 ...
installing: qtconsole-4.2.0-py35_0 ...
installing: qtpy-1.0-py35_0 ...
installing: readline-6.2-2 ...
installing: redis-2.6.9-0 ...
installing: redis-py-2.10.3-py35_0 ...
installing: requests-2.9.1-py35_0 ...
installing: rope-0.9.4-py35_1 ...
installing: scikit-image-0.12.3-np110py35_0 ...
installing: scikit-learn-0.17.1-np110py35_0 ...
installing: scipy-0.17.0-np110py35_2 ...
installing: setuptools-20.3-py35_0 ...
installing: simplegeneric-0.8.1-py35_0 ...
installing: singledispatch-3.4.0.3-py35_0 ...
installing: sip-4.16.9-py35_0 ...
installing: six-1.10.0-py35_0 ...
installing: snowballstemmer-1.2.1-py35_0 ...
installing: sockjs-tornado-1.0.1-py35_0 ...
installing: sphinx-1.3.5-py35_0 ...
installing: sphinx_rtd_theme-0.1.9-py35_0 ...
installing: spyder-2.3.8-py35_1 ...
installing: sqlalchemy-1.0.12-py35_0 ...
installing: sqlite-3.9.2-0 ...
installing: statsmodels-0.6.1-np110py35_0 ...
installing: sympy-1.0-py35_0 ...
installing: terminado-0.5-py35_1 ...
installing: tk-8.5.18-0 ...
installing: toolz-0.7.4-py35_0 ...
installing: tornado-4.3-py35_0 ...
installing: traitlets-4.2.1-py35_0 ...
installing: unicodecsv-0.14.1-py35_0 ...
installing: util-linux-2.21-0 ...
installing: werkzeug-0.11.4-py35_0 ...
installing: wheel-0.29.0-py35_0 ...
installing: xlrd-0.9.4-py35_0 ...
installing: xlsxwriter-0.8.4-py35_0 ...
installing: xlwt-1.0.0-py35_0 ...
installing: xz-5.0.5-1 ...
installing: yaml-0.1.6-0 ...
installing: zeromq-4.1.3-0 ...
installing: zlib-1.2.8-0 ...
installing: anaconda-4.0.0-np110py35_0 ...
installing: conda-4.0.5-py35_0 ...
installing: conda-build-1.20.0-py35_0 ...
installing: conda-env-2.4.5-py35_0 ...
Python 3.5.1 :: Continuum Analytics, Inc.
creating default environment...
installation finished.
Do you wish the installer to prepend the Anaconda3 install location
to PATH in your /home/notroot/.bashrc ? [yes|no]
[no] >>> yes

Prepending PATH=/home/notroot/anaconda3/bin to PATH in /home/notroot/.bashrc
A backup will be made to: /home/notroot/.bashrc-anaconda3.bak


For this change to become active, you have to open a new terminal.

Thank you for installing Anaconda3!

Share your notebooks and packages on Anaconda Cloud!
Sign up for free: https://anaconda.org

notroot@ubuntu:~/downloads$ conda --version
conda: command not found
notroot@ubuntu:~/downloads$ conda --version
conda: command not found
notroot@ubuntu:~/downloads$ export PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH
notroot@ubuntu:~/downloads$ export PYTHONPATH=$SPARK_HOME/python/lib/py4j-0.9-src.zip:$PYTHONPATH
notroot@ubuntu:~/downloads$ conda --version
conda: command not found
notroot@ubuntu:~/downloads$ cd ..
notroot@ubuntu:~$ pwd
/home/notroot
notroot@ubuntu:~$ jps
2511 JobHistoryServer
4978 HistoryServer
1567 NameNode
2066 ResourceManager
7893 Jps
1884 SecondaryNameNode
7779 SparkSubmit
2179 NodeManager
1706 DataNode
notroot@ubuntu:~$
login as: notroot
notroot@192.168.150.143's password:
Welcome to Ubuntu 14.04.1 LTS (GNU/Linux 3.13.0-32-generic x86_64)

 * Documentation:  https://help.ubuntu.com/
New release '16.04.5 LTS' available.
Run 'do-release-upgrade' to upgrade to it.

Last login: Sun Oct 28 22:44:51 2018 from 192.168.150.1
notroot@ubuntu:~$ conda --version
conda 4.0.5
notroot@ubuntu:~$ conda install jupyter
Using Anaconda Cloud api site https://api.anaconda.org
Fetching package metadata: ....
Solving package specifications: .........

Package plan for installation in environment /home/notroot/anaconda3:

The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    conda-env-2.6.0            |                0          502 B
    sqlite-3.13.0              |                0         4.0 MB
    xz-5.2.2                   |                0         644 KB
    python-3.5.4               |                0        15.9 MB
    requests-2.12.4            |           py35_0         800 KB
    ruamel_yaml-0.11.14        |           py35_1         395 KB
    cryptography-1.4           |           py35_0         903 KB
    pyopenssl-16.2.0           |           py35_0          70 KB
    conda-4.3.30               |   py35hf9359ed_0         516 KB
    jupyter-1.0.0              |           py35_3           3 KB
    ------------------------------------------------------------
                                           Total:        23.2 MB

The following NEW packages will be INSTALLED:

    ruamel_yaml:  0.11.14-py35_1

The following packages will be UPDATED:

    conda:        4.0.5-py35_0  --> 4.3.30-py35hf9359ed_0
    conda-env:    2.4.5-py35_0  --> 2.6.0-0
    cryptography: 1.3-py35_0    --> 1.4-py35_0
    jupyter:      1.0.0-py35_2  --> 1.0.0-py35_3
    pyopenssl:    0.15.1-py35_2 --> 16.2.0-py35_0
    python:       3.5.1-0       --> 3.5.4-0
    requests:     2.9.1-py35_0  --> 2.12.4-py35_0
    sqlite:       3.9.2-0       --> 3.13.0-0
    xz:           5.0.5-1       --> 5.2.2-0

Proceed ([y]/n)? y

Fetching packages ...
conda-env-2.6. 100% |################################| Time: 0:00:00 290.86 kB/s
sqlite-3.13.0- 100% |################################| Time: 0:00:03   1.23 MB/s
xz-5.2.2-0.tar 100% |################################| Time: 0:00:00   1.14 MB/s
python-3.5.4-0 100% |################################| Time: 0:00:11   1.45 MB/s
requests-2.12. 100% |################################| Time: 0:00:00   1.23 MB/s
ruamel_yaml-0. 100% |################################| Time: 0:00:00   1.21 MB/s
cryptography-1 100% |################################| Time: 0:00:00   1.68 MB/s
pyopenssl-16.2 100% |################################| Time: 0:00:00   1.38 MB/s
conda-4.3.30-p 100% |################################| Time: 0:00:00   1.59 MB/s
jupyter-1.0.0- 100% |################################| Time: 0:00:00   2.48 MB/s
Extracting packages ...
[      COMPLETE      ]|###################################################| 100%
Unlinking packages ...
[      COMPLETE      ]|###################################################| 100%
Linking packages ...
[      COMPLETE      ]|###################################################| 100%
notroot@ubuntu:~$ jupyter notebook --ip=192.168.150.143
[I 05:26:41.110 NotebookApp] Writing notebook server cookie secret to /run/user/1000/jupyter/notebook_cookie_secret
[I 05:26:41.231 NotebookApp] Serving notebooks from local directory: /home/notroot
[I 05:26:41.232 NotebookApp] 0 active kernels
[I 05:26:41.232 NotebookApp] The Jupyter Notebook is running at: http://192.168.150.143:8888/
[I 05:26:41.232 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[W 05:26:41.233 NotebookApp] No web browser found: could not locate runnable browser.
[I 05:27:27.013 NotebookApp] 302 GET / (192.168.150.1) 0.71ms
[I 05:27:33.827 NotebookApp] Creating new notebook in
[I 05:27:33.833 NotebookApp] Writing notebook-signing key to /home/notroot/.local/share/jupyter/notebook_secret
[I 05:27:34.467 NotebookApp] Kernel started: daef378f-c33f-4ceb-b451-d800d7052158
[I 05:29:34.426 NotebookApp] Saving file at /Untitled.ipynb

